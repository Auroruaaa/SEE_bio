{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee00482d-a526-4179-909e-be1ea64e0fbe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'click'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mclick\u001b[39;00m \u001b[38;5;66;03m# command line interfaces \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01maam\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_parameter_descriptions\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdesc\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'click'"
     ]
    }
   ],
   "source": [
    "import click # command line interfaces \n",
    "import tensorflow as tf\n",
    "import aam._parameter_descriptions as desc\n",
    "from aam.cli_util import aam_model_options # assign default value\n",
    "from attention_regression.data_utils import (\n",
    "    load_biom_table, shuffle_table, filter_and_reorder, extract_col,\n",
    "    convert_table_to_dataset, batch_dataset, convert_to_normalized_dataset,\n",
    "    train_val_split\n",
    ")\n",
    "from attention_regression.model import _construct_model\n",
    "from attention_regression.callbacks import MAE_Scatter\n",
    "from aam.callbacks import SaveModel # X\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9702b99f-9f6e-4853-b1f0-da6ebe7b8cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "@click.group()\n",
    "class cli:\n",
    "    pass\n",
    "\n",
    "\n",
    "def _create_dataset(\n",
    "    i_table_path,\n",
    "    m_metadata_file,\n",
    "    m_metadata_column,\n",
    "    p_normalize,\n",
    "    p_missing_samples\n",
    "):\n",
    "    table = shuffle_table(load_biom_table(i_table_path))\n",
    "    metadata = pd.read_csv(\n",
    "        m_metadata_file, sep='\\t',\n",
    "        index_col=0\n",
    "    )\n",
    "\n",
    "    # check for mismatch samples\n",
    "    ids = table.ids(axis='sample')\n",
    "    shared_ids = set(ids).intersection(set(metadata.index))\n",
    "    min_ids = min(len(shared_ids), len(ids), len(metadata.index))\n",
    "    max_ids = max(len(shared_ids), len(ids), len(metadata.index))\n",
    "    if len(shared_ids) == 0:\n",
    "        raise Exception('Table and Metadata have no matching sample ids')\n",
    "    if min_ids != max_ids and p_missing_samples == 'error':\n",
    "        raise Exception('Table and Metadata do share all same sample ids.')\n",
    "    elif min_ids != max_ids and p_missing_samples == 'ignore':\n",
    "        print('Warning: Table and Metadata do share all same sample ids.')\n",
    "        print('Table and metadata will be filtered')\n",
    "        table = table.filter(shared_ids, inplace=False)\n",
    "        metadata = metadata[metadata.index.isin(shared_ids)]\n",
    "        ids = table.ids(axis='sample')\n",
    "\n",
    "    # TODO: check for invalid metadata values\n",
    "    table = table.remove_empty(axis='observation', inplace=False)\n",
    "    feature_dataset = convert_table_to_dataset(table)\n",
    "    metadata = filter_and_reorder(metadata, ids)\n",
    "    regression_data = extract_col(\n",
    "        metadata,\n",
    "        m_metadata_column,\n",
    "        output_dtype=np.float32\n",
    "    )\n",
    "    regression_dataset, mean, std = convert_to_normalized_dataset(\n",
    "        regression_data,\n",
    "        p_normalize\n",
    "    )\n",
    "    full_dataset = tf.data.Dataset.zip((feature_dataset, regression_dataset))\n",
    "    training, _ = train_val_split(\n",
    "        full_dataset,\n",
    "        train_percent=1.\n",
    "    )\n",
    "    return {\n",
    "        'dataset': training,\n",
    "        'sample_ids': ids,\n",
    "        'mean': mean,\n",
    "        'std': std,\n",
    "        'metadata': metadata,\n",
    "        'table': table\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9083800c-787a-44dd-832a-09368495f353",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cli.command()\n",
    "@click.option(\n",
    "    '--i-table-path',\n",
    "    required=True,\n",
    "    help=desc.TABLE_DESC,\n",
    "    type=click.Path(exists=True)\n",
    ")\n",
    "@click.option(\n",
    "    '--m-metadata-file',\n",
    "    required=True,\n",
    "    type=click.Path(exists=True)\n",
    ")\n",
    "@click.option(\n",
    "    '--m-metadata-column',\n",
    "    required=True,\n",
    "    help=desc.METADATA_COL_DESC,\n",
    "    type=str\n",
    ")\n",
    "@click.option(\n",
    "    '--m-metadata-hue',\n",
    "    default='',\n",
    "    type=str\n",
    ")\n",
    "@click.option(\n",
    "    '--p-normalize',\n",
    "    default='minmax',\n",
    "    type=click.Choice(['minmax', 'z', 'none'])\n",
    ")\n",
    "@click.option(\n",
    "    '--p-missing-samples',\n",
    "    default='error',\n",
    "    type=click.Choice(['error', 'ignore'], case_sensitive=False),\n",
    "    help=desc.MISSING_SAMPLES_DESC\n",
    ")\n",
    "@aam_model_options\n",
    "@click.option(\n",
    "    '--p-output-dir',\n",
    "    required=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a79f2f0-1994-4da9-8bc0-61df611ecb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_regressor(\n",
    "    i_table_path: str,\n",
    "    m_metadata_file: str,\n",
    "    m_metadata_column: str,\n",
    "    m_metadata_hue: str,\n",
    "    p_normalize: str,\n",
    "    p_missing_samples: str,\n",
    "    p_batch_size: int,\n",
    "    p_epochs: int,\n",
    "    p_repeat: int,\n",
    "    p_dropout: float,\n",
    "    p_token_dim: int,\n",
    "    p_feature_attention_method: str,\n",
    "    p_features_to_add_rate: float,\n",
    "    p_ff_d_model: int,\n",
    "    p_ff_clr: int,\n",
    "    p_pca_heads: int,\n",
    "    p_enc_layers: int,\n",
    "    p_enc_heads: int,\n",
    "    p_lr: float,\n",
    "    p_report_back_after: int,\n",
    "    p_output_dir: str\n",
    "):\n",
    "    if not os.path.exists(p_output_dir):\n",
    "        os.makedirs(p_output_dir)\n",
    "\n",
    "    figure_path = os.path.join(p_output_dir, 'figures')\n",
    "    if not os.path.exists(figure_path):\n",
    "        os.makedirs(figure_path)\n",
    "\n",
    "    dataset_obj = _create_dataset(\n",
    "        i_table_path,\n",
    "        m_metadata_file,\n",
    "        m_metadata_column,\n",
    "        p_normalize,\n",
    "        p_missing_samples\n",
    "    )\n",
    "    training = dataset_obj['dataset']\n",
    "    ids = dataset_obj['sample_ids']\n",
    "    training_size = training.cardinality().numpy()\n",
    "    training_ids = ids[:training_size]\n",
    "\n",
    "    training_dataset = batch_dataset(\n",
    "        training,\n",
    "        p_batch_size,\n",
    "        repeat=p_repeat,\n",
    "        shuffle=True\n",
    "    )\n",
    "    training_no_shuffle = batch_dataset(\n",
    "        training,\n",
    "        p_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    table = dataset_obj['table']\n",
    "    mean = dataset_obj['mean']\n",
    "    std = dataset_obj['std']\n",
    "    metadata = dataset_obj['metadata']\n",
    "    model = _construct_model(\n",
    "        table.ids(axis='observation').tolist(),\n",
    "        mean,\n",
    "        std,\n",
    "        p_token_dim,\n",
    "        p_feature_attention_method,\n",
    "        p_features_to_add_rate,\n",
    "        p_dropout,\n",
    "        p_ff_clr,\n",
    "        p_ff_d_model,\n",
    "        p_pca_heads,\n",
    "        p_enc_layers,\n",
    "        p_enc_heads,\n",
    "        p_lr\n",
    "    )\n",
    "    for x, y in training_dataset.take(1):\n",
    "        model((x['feature'], x['rclr']))\n",
    "    model.summary()\n",
    "\n",
    "    reg_out_callbacks = [\n",
    "        MAE_Scatter(\n",
    "            'training',\n",
    "            training_no_shuffle,\n",
    "            metadata[metadata.index.isin(training_ids)],\n",
    "            m_metadata_column,\n",
    "            m_metadata_hue,\n",
    "            m_metadata_hue,\n",
    "            mean,\n",
    "            std,\n",
    "            figure_path,\n",
    "            report_back_after=p_report_back_after\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    core_callbacks = [\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            \"loss\",\n",
    "            factor=0.5,\n",
    "            patients=2,\n",
    "            min_lr=0.000001\n",
    "        ),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            'loss',\n",
    "            patience=50\n",
    "        ),\n",
    "        SaveModel(p_output_dir)\n",
    "    ]\n",
    "    model.fit(\n",
    "        training_dataset,\n",
    "        callbacks=[\n",
    "            *reg_out_callbacks,\n",
    "            *core_callbacks\n",
    "        ],\n",
    "        epochs=p_epochs\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209b5663-7369-46aa-a071-a0ef3c521cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cli.command()\n",
    "@click.option(\n",
    "    '--i-table-path',\n",
    "    required=True,\n",
    "    help=desc.TABLE_DESC,\n",
    "    type=click.Path(exists=True)\n",
    ")\n",
    "@click.option(\n",
    "    '--i-model-path',\n",
    "    required=True,\n",
    "    type=click.Path(exists=True)\n",
    ")\n",
    "@click.option(\n",
    "    '--m-metadata-file',\n",
    "    required=True,\n",
    "    type=click.Path(exists=True)\n",
    ")\n",
    "@click.option(\n",
    "    '--m-metadata-column',\n",
    "    required=True,\n",
    "    help=desc.METADATA_COL_DESC,\n",
    "    type=str\n",
    ")\n",
    "@click.option(\n",
    "    '--p-normalize',\n",
    "    default='minmax',\n",
    "    type=click.Choice(['minmax', 'z', 'none'])\n",
    ")\n",
    "@click.option(\n",
    "    '--p-missing-samples',\n",
    "    default='error',\n",
    "    type=click.Choice(['error', 'ignore'], case_sensitive=False),\n",
    "    help=desc.MISSING_SAMPLES_DESC\n",
    ")\n",
    "@click.option(\n",
    "    '--p-output-dir',\n",
    "    required=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813edbab-c99b-4012-99fe-b707d1b9ee0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot(\n",
    "    i_table_path,\n",
    "    i_model_path,\n",
    "    m_metadata_file,\n",
    "    m_metadata_column,\n",
    "    p_normalize,\n",
    "    p_missing_samples,\n",
    "    p_output_dir\n",
    "):\n",
    "    if not os.path.exists(p_output_dir):\n",
    "        os.makedirs(p_output_dir)\n",
    "    model = tf.keras.models.load_model(i_model_path)\n",
    "    dataset_obj = _create_dataset(\n",
    "        i_table_path,\n",
    "        m_metadata_file,\n",
    "        m_metadata_column,\n",
    "        p_normalize,\n",
    "        p_missing_samples\n",
    "    )\n",
    "    training = dataset_obj['dataset']\n",
    "    training_no_shuffle = batch_dataset(\n",
    "        training,\n",
    "        32,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    mean = dataset_obj['mean']\n",
    "    std = dataset_obj['std']\n",
    "    std = model.std\n",
    "    mean = model.mean\n",
    "    output = model.predict(training_no_shuffle)\n",
    "    pred_val = tf.concat(output['regression'], axis=0)\n",
    "    pred_val = tf.squeeze(pred_val)\n",
    "    pred_val = pred_val*std + mean\n",
    "    true_val = tf.concat(\n",
    "        [y[\"reg_out\"] for _, y in training_no_shuffle],\n",
    "        axis=0\n",
    "    )\n",
    "    true_val = tf.squeeze(true_val)\n",
    "    true_val = true_val*std + mean\n",
    "    mae = tf.reduce_mean(tf.abs(true_val - pred_val))\n",
    "\n",
    "    min_x = tf.reduce_min(true_val)\n",
    "    max_x = tf.reduce_max(true_val)\n",
    "    coeff = np.polyfit(true_val, pred_val, deg=1)\n",
    "    p = np.poly1d(coeff)\n",
    "    xx = np.linspace(min_x, max_x, 50)\n",
    "    yy = p(xx)\n",
    "\n",
    "    diag = np.polyfit(true_val, true_val, deg=1)\n",
    "    p = np.poly1d(diag)\n",
    "    diag_xx = np.linspace(min_x, max_x, 50)\n",
    "    diag_yy = p(diag_xx)\n",
    "    data = {\n",
    "        \"#SampleID\": dataset_obj[\"sample_ids\"],\n",
    "        \"pred\": pred_val.numpy(),\n",
    "        \"true\": true_val.numpy()\n",
    "    }\n",
    "    data = pd.DataFrame(data=data)\n",
    "    plot = sns.scatterplot(data, x=\"true\", y=\"pred\")\n",
    "    plt.plot(xx, yy)\n",
    "    plt.plot(diag_xx, diag_yy)\n",
    "    mae = '%.4g' % mae\n",
    "    plot.set(xlabel='True')\n",
    "    plot.set(ylabel='Predicted')\n",
    "    plot.set(title=f\"Mean Absolute Error: {mae}\")\n",
    "    plt.savefig(\n",
    "        os.path.join(p_output_dir, 'scatter-plot.png'),\n",
    "        bbox_inches=\"tight\"\n",
    "    )\n",
    "    plt.close()\n",
    "    data[\"residual\"] = data[\"true\"] - data[\"pred\"]\n",
    "\n",
    "    mean_residual = np.mean(np.abs(data[\"residual\"]))\n",
    "    mean_residual = '%.4g' % mean_residual\n",
    "    plot = sns.displot(data, x=\"residual\")\n",
    "    plot.set(title=f\"Mean Absolute Residual: {mean_residual}\")\n",
    "    plt.savefig(\n",
    "        os.path.join(p_output_dir, 'residual-plot.png'),\n",
    "        bbox_inches=\"tight\"\n",
    "    )\n",
    "    plt.close()\n",
    "    data.to_csv(\n",
    "        os.path.join(p_output_dir, 'sample-residuals.tsv'),\n",
    "        sep='\\t',\n",
    "        index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebb3779-c107-4d08-b3ab-fcaaf089e860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if len(gpus) > 0:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    cli()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

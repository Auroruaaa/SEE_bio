{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee00482d-a526-4179-909e-be1ea64e0fbe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maam\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcli_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m aam_model_options \u001b[38;5;66;03m# assign default value\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mattention_regression\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      6\u001b[0m     load_biom_table, shuffle_table, filter_and_reorder, extract_col,\n\u001b[1;32m      7\u001b[0m     convert_table_to_dataset, batch_dataset, convert_to_normalized_dataset,\n\u001b[1;32m      8\u001b[0m     train_val_split\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mattention_regression\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _construct_model\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mattention_regression\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MAE_Scatter\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maam\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SaveModel \u001b[38;5;66;03m# X\u001b[39;00m\n",
      "File \u001b[0;32m~/attention-all-microbes/attention_regression/model.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mattention_regression\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     FeatureEmbedding,\n\u001b[1;32m      4\u001b[0m     FeatureLoadings,\n\u001b[1;32m      5\u001b[0m     Regressor\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maam\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MAE\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# temp hacky way to initialize model creation\u001b[39;00m\n",
      "File \u001b[0;32m~/attention-all-microbes/attention_regression/layers.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_models\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfm\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39msaving\u001b[38;5;241m.\u001b[39mregister_keras_serializable(\n\u001b[1;32m      6\u001b[0m     package\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeatureEmbedding\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFeatureEmbedding\u001b[39;00m(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mLayer):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     11\u001b[0m         token_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     19\u001b[0m     ):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_models'"
     ]
    }
   ],
   "source": [
    "import click # command line interfaces \n",
    "import tensorflow as tf\n",
    "import aam._parameter_descriptions as desc\n",
    "from aam.cli_util import aam_model_options # assign default value\n",
    "from attention_regression.data_utils import (\n",
    "    load_biom_table, shuffle_table, filter_and_reorder, extract_col,\n",
    "    convert_table_to_dataset, batch_dataset, convert_to_normalized_dataset,\n",
    "    train_val_split\n",
    ")\n",
    "from attention_regression.model import _construct_model\n",
    "from attention_regression.callbacks import MAE_Scatter\n",
    "from aam.callbacks import SaveModel # X\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os # Miscellaneous operating system interfaces\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503c57db-aee2-49a7-ad81-a7006e00e009",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acb33875-665b-4877-8391-5e4fa31dc87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting biom-format\n",
      "  Downloading biom-format-2.1.16.tar.gz (11.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from biom-format) (8.1.7)\n",
      "Requirement already satisfied: numpy>=1.9.2 in /opt/conda/lib/python3.11/site-packages (from biom-format) (1.26.1)\n",
      "Requirement already satisfied: scipy>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from biom-format) (1.11.3)\n",
      "Collecting pandas>=0.20.0 (from biom-format)\n",
      "  Obtaining dependency information for pandas>=0.20.0 from https://files.pythonhosted.org/packages/fc/a5/4d82be566f069d7a9a702dcdf6f9106df0e0b042e738043c0cc7ddd7e3f6/pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.11/site-packages (from biom-format) (3.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas>=0.20.0->biom-format) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=0.20.0->biom-format) (2023.3.post1)\n",
      "Collecting tzdata>=2022.7 (from pandas>=0.20.0->biom-format)\n",
      "  Obtaining dependency information for tzdata>=2022.7 from https://files.pythonhosted.org/packages/65/58/f9c9e6be752e9fcb8b6a0ee9fb87e6e7a1f6bcab2cdc73f02bb7ba91ada0/tzdata-2024.1-py2.py3-none-any.whl.metadata\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=0.20.0->biom-format) (1.16.0)\n",
      "Using cached pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Building wheels for collected packages: biom-format\n",
      "  Building wheel for biom-format (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for biom-format: filename=biom_format-2.1.16-cp311-cp311-linux_x86_64.whl size=11803646 sha256=a3e7349088f9ac1a1abe456e96523f0d9c3100c89b4f024bd1c1e0fecc4daf37\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/a5/6b/58/a879e8fbae2479a3d1a68719f3a062fe62701d6494f1b74f5e\n",
      "Successfully built biom-format\n",
      "Installing collected packages: tzdata, pandas, biom-format\n",
      "Successfully installed biom-format-2.1.16 pandas-2.2.2 tzdata-2024.1\n"
     ]
    }
   ],
   "source": [
    "!pip install biom-format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2203daa7-58d4-4e06-8d52-63d7a312b54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement cikit-bio (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for cikit-bio\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.26.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.11/site-packages (1.11.3)\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /opt/conda/lib/python3.11/site-packages (from scipy) (1.26.1)\n",
      "Collecting unifrac\n",
      "  Downloading unifrac-0.20.3.tar.gz (258 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.0/259.0 kB\u001b[0m \u001b[31m385.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[6 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-bgn_gmt8/unifrac_97f5adb5ac534cf4b0e5718e19af37fa/setup.py\", line 94, in <module>\n",
      "  \u001b[31m   \u001b[0m     from Cython.Build import cythonize\n",
      "  \u001b[31m   \u001b[0m ModuleNotFoundError: No module named 'Cython'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install cikit-bio\n",
    "!pip install scikit-learn\n",
    "!pip install scipy\n",
    "!pip install unifrac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9702b99f-9f6e-4853-b1f0-da6ebe7b8cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "@click.group()\n",
    "class cli:\n",
    "    pass\n",
    "\n",
    "\n",
    "def _create_dataset(\n",
    "    i_table_path,\n",
    "    m_metadata_file,\n",
    "    m_metadata_column,\n",
    "    p_normalize,\n",
    "    p_missing_samples\n",
    "):\n",
    "    table = shuffle_table(load_biom_table(i_table_path))\n",
    "    metadata = pd.read_csv(\n",
    "        m_metadata_file, sep='\\t',\n",
    "        index_col=0\n",
    "    )\n",
    "\n",
    "    # check for mismatch samples\n",
    "    ids = table.ids(axis='sample')\n",
    "    shared_ids = set(ids).intersection(set(metadata.index))\n",
    "    min_ids = min(len(shared_ids), len(ids), len(metadata.index))\n",
    "    max_ids = max(len(shared_ids), len(ids), len(metadata.index))\n",
    "    if len(shared_ids) == 0:\n",
    "        raise Exception('Table and Metadata have no matching sample ids')\n",
    "    if min_ids != max_ids and p_missing_samples == 'error':\n",
    "        raise Exception('Table and Metadata do share all same sample ids.')\n",
    "    elif min_ids != max_ids and p_missing_samples == 'ignore':\n",
    "        print('Warning: Table and Metadata do share all same sample ids.')\n",
    "        print('Table and metadata will be filtered')\n",
    "        table = table.filter(shared_ids, inplace=False)\n",
    "        metadata = metadata[metadata.index.isin(shared_ids)]\n",
    "        ids = table.ids(axis='sample')\n",
    "\n",
    "    # TODO: check for invalid metadata values\n",
    "    table = table.remove_empty(axis='observation', inplace=False)\n",
    "    feature_dataset = convert_table_to_dataset(table)\n",
    "    metadata = filter_and_reorder(metadata, ids)\n",
    "    regression_data = extract_col(\n",
    "        metadata,\n",
    "        m_metadata_column,\n",
    "        output_dtype=np.float32\n",
    "    )\n",
    "    regression_dataset, mean, std = convert_to_normalized_dataset(\n",
    "        regression_data,\n",
    "        p_normalize\n",
    "    )\n",
    "    full_dataset = tf.data.Dataset.zip((feature_dataset, regression_dataset))\n",
    "    training, _ = train_val_split(\n",
    "        full_dataset,\n",
    "        train_percent=1.\n",
    "    )\n",
    "    return {\n",
    "        'dataset': training,\n",
    "        'sample_ids': ids,\n",
    "        'mean': mean,\n",
    "        'std': std,\n",
    "        'metadata': metadata,\n",
    "        'table': table\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9083800c-787a-44dd-832a-09368495f353",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cli.command()\n",
    "@click.option(\n",
    "    '--i-table-path',\n",
    "    required=True,\n",
    "    help=desc.TABLE_DESC,\n",
    "    type=click.Path(exists=True)\n",
    ")\n",
    "@click.option(\n",
    "    '--m-metadata-file',\n",
    "    required=True,\n",
    "    type=click.Path(exists=True)\n",
    ")\n",
    "@click.option(\n",
    "    '--m-metadata-column',\n",
    "    required=True,\n",
    "    help=desc.METADATA_COL_DESC,\n",
    "    type=str\n",
    ")\n",
    "@click.option(\n",
    "    '--m-metadata-hue',\n",
    "    default='',\n",
    "    type=str\n",
    ")\n",
    "@click.option(\n",
    "    '--p-normalize',\n",
    "    default='minmax',\n",
    "    type=click.Choice(['minmax', 'z', 'none'])\n",
    ")\n",
    "@click.option(\n",
    "    '--p-missing-samples',\n",
    "    default='error',\n",
    "    type=click.Choice(['error', 'ignore'], case_sensitive=False),\n",
    "    help=desc.MISSING_SAMPLES_DESC\n",
    ")\n",
    "@aam_model_options\n",
    "@click.option(\n",
    "    '--p-output-dir',\n",
    "    required=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a79f2f0-1994-4da9-8bc0-61df611ecb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_regressor(\n",
    "    i_table_path: str,\n",
    "    m_metadata_file: str,\n",
    "    m_metadata_column: str,\n",
    "    m_metadata_hue: str,\n",
    "    p_normalize: str,\n",
    "    p_missing_samples: str,\n",
    "    p_batch_size: int,\n",
    "    p_epochs: int,\n",
    "    p_repeat: int,\n",
    "    p_dropout: float,\n",
    "    p_token_dim: int,\n",
    "    p_feature_attention_method: str,\n",
    "    p_features_to_add_rate: float,\n",
    "    p_ff_d_model: int,\n",
    "    p_ff_clr: int,\n",
    "    p_pca_heads: int,\n",
    "    p_enc_layers: int,\n",
    "    p_enc_heads: int,\n",
    "    p_lr: float,\n",
    "    p_report_back_after: int,\n",
    "    p_output_dir: str\n",
    "):\n",
    "    if not os.path.exists(p_output_dir):\n",
    "        os.makedirs(p_output_dir)\n",
    "\n",
    "    figure_path = os.path.join(p_output_dir, 'figures')\n",
    "    if not os.path.exists(figure_path):\n",
    "        os.makedirs(figure_path)\n",
    "\n",
    "    dataset_obj = _create_dataset(\n",
    "        i_table_path,\n",
    "        m_metadata_file,\n",
    "        m_metadata_column,\n",
    "        p_normalize,\n",
    "        p_missing_samples\n",
    "    )\n",
    "    training = dataset_obj['dataset']\n",
    "    ids = dataset_obj['sample_ids']\n",
    "    training_size = training.cardinality().numpy()\n",
    "    training_ids = ids[:training_size]\n",
    "\n",
    "    training_dataset = batch_dataset(\n",
    "        training,\n",
    "        p_batch_size,\n",
    "        repeat=p_repeat,\n",
    "        shuffle=True\n",
    "    )\n",
    "    training_no_shuffle = batch_dataset(\n",
    "        training,\n",
    "        p_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    table = dataset_obj['table']\n",
    "    mean = dataset_obj['mean']\n",
    "    std = dataset_obj['std']\n",
    "    metadata = dataset_obj['metadata']\n",
    "    model = _construct_model(\n",
    "        table.ids(axis='observation').tolist(),\n",
    "        mean,\n",
    "        std,\n",
    "        p_token_dim,\n",
    "        p_feature_attention_method,\n",
    "        p_features_to_add_rate,\n",
    "        p_dropout,\n",
    "        p_ff_clr,\n",
    "        p_ff_d_model,\n",
    "        p_pca_heads,\n",
    "        p_enc_layers,\n",
    "        p_enc_heads,\n",
    "        p_lr\n",
    "    )\n",
    "    for x, y in training_dataset.take(1):\n",
    "        model((x['feature'], x['rclr']))\n",
    "    model.summary()\n",
    "\n",
    "    reg_out_callbacks = [\n",
    "        MAE_Scatter(\n",
    "            'training',\n",
    "            training_no_shuffle,\n",
    "            metadata[metadata.index.isin(training_ids)],\n",
    "            m_metadata_column,\n",
    "            m_metadata_hue,\n",
    "            m_metadata_hue,\n",
    "            mean,\n",
    "            std,\n",
    "            figure_path,\n",
    "            report_back_after=p_report_back_after\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    core_callbacks = [\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            \"loss\",\n",
    "            factor=0.5,\n",
    "            patients=2,\n",
    "            min_lr=0.000001\n",
    "        ),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            'loss',\n",
    "            patience=50\n",
    "        ),\n",
    "        SaveModel(p_output_dir)\n",
    "    ]\n",
    "    model.fit(\n",
    "        training_dataset,\n",
    "        callbacks=[\n",
    "            *reg_out_callbacks,\n",
    "            *core_callbacks\n",
    "        ],\n",
    "        epochs=p_epochs\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "209b5663-7369-46aa-a071-a0ef3c521cb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1722307965.py, line 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 38\u001b[0;36m\u001b[0m\n\u001b[0;31m    )\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "@cli.command()\n",
    "@click.option(\n",
    "    '--i-table-path',\n",
    "    required=True,\n",
    "    help=desc.TABLE_DESC,\n",
    "    type=click.Path(exists=True)\n",
    ")\n",
    "@click.option(\n",
    "    '--i-model-path',\n",
    "    required=True,\n",
    "    type=click.Path(exists=True)\n",
    ")\n",
    "@click.option(\n",
    "    '--m-metadata-file',\n",
    "    required=True,\n",
    "    type=click.Path(exists=True)\n",
    ")\n",
    "@click.option(\n",
    "    '--m-metadata-column',\n",
    "    required=True,\n",
    "    help=desc.METADATA_COL_DESC,\n",
    "    type=str\n",
    ")\n",
    "@click.option(\n",
    "    '--p-normalize',\n",
    "    default='minmax',\n",
    "    type=click.Choice(['minmax', 'z', 'none'])\n",
    ")\n",
    "@click.option(\n",
    "    '--p-missing-samples',\n",
    "    default='error',\n",
    "    type=click.Choice(['error', 'ignore'], case_sensitive=False),\n",
    "    help=desc.MISSING_SAMPLES_DESC\n",
    ")\n",
    "@click.option(\n",
    "    '--p-output-dir',\n",
    "    required=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813edbab-c99b-4012-99fe-b707d1b9ee0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot(\n",
    "    i_table_path,\n",
    "    i_model_path,\n",
    "    m_metadata_file,\n",
    "    m_metadata_column,\n",
    "    p_normalize,\n",
    "    p_missing_samples,\n",
    "    p_output_dir\n",
    "):\n",
    "    if not os.path.exists(p_output_dir):\n",
    "        os.makedirs(p_output_dir)\n",
    "    model = tf.keras.models.load_model(i_model_path)\n",
    "    dataset_obj = _create_dataset(\n",
    "        i_table_path,\n",
    "        m_metadata_file,\n",
    "        m_metadata_column,\n",
    "        p_normalize,\n",
    "        p_missing_samples\n",
    "    )\n",
    "    training = dataset_obj['dataset']\n",
    "    training_no_shuffle = batch_dataset(\n",
    "        training,\n",
    "        32,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    mean = dataset_obj['mean']\n",
    "    std = dataset_obj['std']\n",
    "    std = model.std\n",
    "    mean = model.mean\n",
    "    output = model.predict(training_no_shuffle)\n",
    "    pred_val = tf.concat(output['regression'], axis=0)\n",
    "    pred_val = tf.squeeze(pred_val)\n",
    "    pred_val = pred_val*std + mean\n",
    "    true_val = tf.concat(\n",
    "        [y[\"reg_out\"] for _, y in training_no_shuffle],\n",
    "        axis=0\n",
    "    )\n",
    "    true_val = tf.squeeze(true_val)\n",
    "    true_val = true_val*std + mean\n",
    "    mae = tf.reduce_mean(tf.abs(true_val - pred_val))\n",
    "\n",
    "    min_x = tf.reduce_min(true_val)\n",
    "    max_x = tf.reduce_max(true_val)\n",
    "    coeff = np.polyfit(true_val, pred_val, deg=1)\n",
    "    p = np.poly1d(coeff)\n",
    "    xx = np.linspace(min_x, max_x, 50)\n",
    "    yy = p(xx)\n",
    "\n",
    "    diag = np.polyfit(true_val, true_val, deg=1)\n",
    "    p = np.poly1d(diag)\n",
    "    diag_xx = np.linspace(min_x, max_x, 50)\n",
    "    diag_yy = p(diag_xx)\n",
    "    data = {\n",
    "        \"#SampleID\": dataset_obj[\"sample_ids\"],\n",
    "        \"pred\": pred_val.numpy(),\n",
    "        \"true\": true_val.numpy()\n",
    "    }\n",
    "    data = pd.DataFrame(data=data)\n",
    "    plot = sns.scatterplot(data, x=\"true\", y=\"pred\")\n",
    "    plt.plot(xx, yy)\n",
    "    plt.plot(diag_xx, diag_yy)\n",
    "    mae = '%.4g' % mae\n",
    "    plot.set(xlabel='True')\n",
    "    plot.set(ylabel='Predicted')\n",
    "    plot.set(title=f\"Mean Absolute Error: {mae}\")\n",
    "    plt.savefig(\n",
    "        os.path.join(p_output_dir, 'scatter-plot.png'),\n",
    "        bbox_inches=\"tight\"\n",
    "    )\n",
    "    plt.close()\n",
    "    data[\"residual\"] = data[\"true\"] - data[\"pred\"]\n",
    "\n",
    "    mean_residual = np.mean(np.abs(data[\"residual\"]))\n",
    "    mean_residual = '%.4g' % mean_residual\n",
    "    plot = sns.displot(data, x=\"residual\")\n",
    "    plot.set(title=f\"Mean Absolute Residual: {mean_residual}\")\n",
    "    plt.savefig(\n",
    "        os.path.join(p_output_dir, 'residual-plot.png'),\n",
    "        bbox_inches=\"tight\"\n",
    "    )\n",
    "    plt.close()\n",
    "    data.to_csv(\n",
    "        os.path.join(p_output_dir, 'sample-residuals.tsv'),\n",
    "        sep='\\t',\n",
    "        index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebb3779-c107-4d08-b3ab-fcaaf089e860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if len(gpus) > 0:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    cli()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

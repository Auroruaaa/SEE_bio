{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c47f2616-6dc7-455d-aad3-88bac044d335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import biom\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from biom import Table\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from skbio.stats.ordination import pcoa\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3b85fd4-d8db-4b43-bfe1-26e23a2d4865",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f874e247-64c9-4e1c-9fe1-85451bbb0f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_path = \"./redbiom_adrc_wolr2_fecal_v2.biom\"\n",
    "meta_path = \"./redbiom_adrc_wolr2_fecal_v2.tsv\"\n",
    "\n",
    "table = biom.load_table(table_path)\n",
    "\n",
    "meta = pd.read_csv(meta_path, sep='\\t')\n",
    "\n",
    "# print(meta.shape)\n",
    "# print(meta.head(5))\n",
    "# print(f\"Table shape: {table.shape}\")\n",
    "# print(table.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3250c2f4-4c0e-45ec-944d-4e809cc33472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the BIOM table to a pandas DataFrame (OTUs as rows, samples as columns)\n",
    "biom_df = pd.DataFrame(table.matrix_data.toarray(), index=table.ids(axis='observation'), columns=table.ids(axis='sample'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a179e07-837b-4ddd-87ce-f4f561813a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of binary features: 1568\n",
      "Number of numerical features: 13795\n"
     ]
    }
   ],
   "source": [
    "def classify_feature(row):\n",
    "    unique_values = row.unique()\n",
    "    if len(unique_values) == 2 and set(unique_values).issubset({0, 1}):\n",
    "        return 'binary'\n",
    "    else:\n",
    "        return 'numerical'\n",
    "\n",
    "biom_df['feature_type'] = biom_df.apply(classify_feature, axis=1)\n",
    "\n",
    "binary_features = biom_df[biom_df['feature_type'] == 'binary']\n",
    "numerical_features = biom_df[biom_df['feature_type'] == 'numerical']\n",
    "\n",
    "binary_features = binary_features.drop(columns=['feature_type'])\n",
    "numerical_features = numerical_features.drop(columns=['feature_type'])\n",
    "\n",
    "print(f\"Number of binary features: {binary_features.shape[0]}\")\n",
    "print(f\"Number of numerical features: {numerical_features.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9361d947-cce6-408b-8c13-9fd008aa1173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Check general information about the table\n",
    "# print(\"Table dimensions (rows, columns):\", biom_df.shape)\n",
    "# print(\"Maximum value in the table:\", biom_df.values.max())\n",
    "# print(\"Minimum value in the table:\", biom_df.values.min())\n",
    "# print(\"Mean value in the table:\", biom_df.values.mean())\n",
    "# print(\"Standard deviation in the table:\", biom_df.values.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "078b0409-e12c-4644-ae1b-d87b1773bcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = numerical_features.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86556f20-b2cf-4160-adb9-c6e2c8a56dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_d = numerical_features.copy()\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaled_d[numerical_features.columns] = scaler.fit_transform(numerical_features)\n",
    "scaled = scaler.fit_transform(num_data)\n",
    "\n",
    "# print(\"Scaled Numerical Dataset:\")\n",
    "# print(scaled.head())\n",
    "# print(scaled.shape) # (13795, 13436)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ff4cbab-d3f3-4c7e-851f-ce94fd9a1195",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_2 = MinMaxScaler()\n",
    "scaled_2 = scaler_2.fit_transform(num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eded7618-20f9-405b-befe-1a148b8d9ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_1 = StandardScaler()\n",
    "scaled_1 = scaler_1.fit_transform(num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2a7c378-1624-4791-86ae-ad47bc1920dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13436, 13795)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7d28764-96bd-4a85-af19-0bdf7a4da26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = scaled[~np.isnan(scaled).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61616330-c69c-44be-bb71-8011a465b8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13436, 13795)\n",
      "(13436, 13795)\n"
     ]
    }
   ],
   "source": [
    "print(scaled.shape)\n",
    "print(clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab122b13-b1cc-433e-9d06-7d15636e02ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1727713597.131678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/skbio/stats/ordination/_principal_coordinate_analysis.py:146: RuntimeWarning: The result contains negative eigenvalues. Please compare their magnitude with the magnitude of some of the largest positive eigenvalues. If the negative ones are smaller, it's probably safe to ignore them, but if they are large in magnitude, the results won't be useful. See the Notes section for more details. The smallest eigenvalue is -0.2999125480552377 and the largest is 4059092728681242.5.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                PC1            PC2            PC3            PC4  \\\n",
      "0     -3.248695e+06 -576371.445652 -607291.079868  511376.074966   \n",
      "1     -3.115778e+02   39123.159677   60486.508880  -70207.490780   \n",
      "2      1.436833e+05  -48037.821287   13673.230721   -1633.312265   \n",
      "3      1.127946e+05   76440.615929   -6093.942982  -16852.402048   \n",
      "4      1.707834e+05  157426.647613  -92674.275936   50247.424616   \n",
      "...             ...            ...            ...            ...   \n",
      "13431 -3.791729e+05  -91210.138215 -123400.219649  180843.622730   \n",
      "13432 -2.704470e+05  -54108.470951 -126675.877762   83746.482084   \n",
      "13433 -7.597396e+05   89641.608099   35056.912698  -53501.198762   \n",
      "13434 -9.806089e+05 -119857.658340 -247686.524320  184542.014154   \n",
      "13435 -7.683849e+05  100479.157638  -92696.145454  -36011.426261   \n",
      "\n",
      "                 PC5           PC6           PC7            PC8  \\\n",
      "0      -74201.107154  2.679797e+05  3.506847e+05  234849.169595   \n",
      "1      -93666.208706  2.378098e+05 -2.841562e+04  143402.706447   \n",
      "2       33036.403649  6.065791e+03 -1.224040e+04   15910.551899   \n",
      "3      -14179.816407  2.575212e+04  1.001273e+05    1477.752027   \n",
      "4       20397.694347 -5.165363e+04  2.008918e+04    5846.729137   \n",
      "...              ...           ...           ...            ...   \n",
      "13431  -15362.300776  3.149308e+05  8.578648e+04   79364.588483   \n",
      "13432  -26399.510437  4.315308e+05 -3.516223e+04 -222456.517735   \n",
      "13433 -220797.620756  6.917789e+05  6.351132e+05 -233880.133093   \n",
      "13434 -118220.917252  6.415248e+05  5.019981e+05  168119.520355   \n",
      "13435 -146289.480806  1.311389e+06  1.077179e+06  315576.022418   \n",
      "\n",
      "                 PC9           PC10  ...         PC119          PC120  \\\n",
      "0      261649.676916  -40661.700213  ...  42589.931080  149093.698186   \n",
      "1       14945.535240  -44630.802604  ...  15324.442400   22441.836923   \n",
      "2      -31188.842041   31687.140760  ...  -1930.448830    7963.881976   \n",
      "3      -22020.564971   18597.876495  ...   1086.325946   -8078.063841   \n",
      "4        -350.495697   -4525.720935  ...  -2657.962686     430.054074   \n",
      "...              ...            ...  ...           ...            ...   \n",
      "13431  -34056.090601   20459.663234  ...  18216.775473    4143.069959   \n",
      "13432  215774.185721  215009.532753  ...  77864.685686  -13829.272951   \n",
      "13433  144499.028131  355051.181549  ...  43018.553185    9839.676728   \n",
      "13434 -129260.974899  -15619.511482  ...   6372.878603  -13155.152462   \n",
      "13435   10163.986826 -156937.570532  ... -56165.959697    3751.396664   \n",
      "\n",
      "              PC121         PC122          PC123         PC124         PC125  \\\n",
      "0      87891.422075 -70699.005626   25825.938954 -47801.064418  51775.478103   \n",
      "1      12475.487658  14148.668863    9161.739956 -20219.143555   2234.678061   \n",
      "2      -2201.268317  -6102.972748    1181.760863    621.697770   6368.869629   \n",
      "3      -7221.600423   -777.856989    -987.674796   -575.018442  -2595.963781   \n",
      "4      -3417.782585    914.510636   -3162.659178  -2059.106220  -2130.824467   \n",
      "...             ...           ...            ...           ...           ...   \n",
      "13431  16965.003250   3159.684045  -14751.004814   2906.701528 -10770.850336   \n",
      "13432 -35392.501955  -5739.775244   59774.433504  -1917.497749  43415.726555   \n",
      "13433  22838.402518   6872.064026    7498.008935 -56320.216039  -2716.083744   \n",
      "13434 -14211.477825 -12474.908071  100180.110275  38063.667820 -17877.706279   \n",
      "13435   2189.902330  76868.225191  -24273.460600  34479.193013 -15631.672871   \n",
      "\n",
      "              PC126         PC127         PC128  \n",
      "0       5389.636817 -99891.380125 -31793.477851  \n",
      "1       8645.544117 -10761.582540  21351.421888  \n",
      "2       1746.748235   7476.594337   -594.984117  \n",
      "3       8068.208848  -5982.228088  -5815.418513  \n",
      "4      -1115.772542  -1707.212850    -67.163972  \n",
      "...             ...           ...           ...  \n",
      "13431 -26101.511280  -4537.839630  18668.282614  \n",
      "13432  11657.503287 -81592.474307  78706.498645  \n",
      "13433 -19026.322870  15895.228720  14531.271682  \n",
      "13434 -48237.848830  30194.430847  10554.452465  \n",
      "13435 -43093.286227  48684.074100  -8451.712748  \n",
      "\n",
      "[13436 rows x 128 columns]\n",
      "Ordination results:\n",
      "\tMethod: Principal Coordinate Analysis (PCoA)\n",
      "\tEigvals: 13436\n",
      "\tProportion explained: 13436\n",
      "\tFeatures: N/A\n",
      "\tSamples: 13436x13436\n",
      "\tBiplot Scores: N/A\n",
      "\tSample constraints: N/A\n",
      "\tFeature IDs: N/A\n",
      "\tSample IDs: '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', ...\n",
      "Execution time: 125.61435508728027 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nNote: runtime has no connection with output dimension. Dim only affects how many rows we want from the result.\\nDim = 16, Execution time: 149.83883666992188 seconds\\nDim = 64, Execution time: 226.11388063430786 seconds\\nDim = 128, Execution time: 147.53481197357178 seconds\\nDim = 128, Execution time: 150.34863781929016 seconds\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "print(start_time)\n",
    "\n",
    "df_scaled = pd.DataFrame(scaled)\n",
    "\n",
    "dm = pairwise_distances(num_data, metric='euclidean')\n",
    "# dm = pairwise_distances(scaled_2, metric='euclidean')\n",
    "# dm = (dm + dm.T) / 2\n",
    "\n",
    "# dm = pairwise_distances(scaled_2, metric='euclidean')\n",
    "\n",
    "\n",
    "dim = 128\n",
    "\n",
    "pcoa_results = pcoa(dm)\n",
    "reduced = pcoa_results.samples.iloc[:, :dim]\n",
    "print(reduced)\n",
    "\n",
    "\n",
    "# View the PCoA results\n",
    "print(pcoa_results)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")\n",
    "'''\n",
    "Note: runtime has no connection with output dimension. Dim only affects how many rows we want from the result.\n",
    "Dim = 16, Execution time: 149.83883666992188 seconds\n",
    "Dim = 64, Execution time: 226.11388063430786 seconds\n",
    "Dim = 128, Execution time: 147.53481197357178 seconds\n",
    "Dim = 128, Execution time: 150.34863781929016 seconds\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3571ea0a-0d35-4570-b85e-384853c1a116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC1        0.325234\n",
      "PC2        0.085104\n",
      "PC3        0.077676\n",
      "PC4        0.068997\n",
      "PC5        0.044542\n",
      "             ...   \n",
      "PC13432    0.000000\n",
      "PC13433    0.000000\n",
      "PC13434    0.000000\n",
      "PC13435    0.000000\n",
      "PC13436    0.000000\n",
      "Length: 13436, dtype: float64\n",
      "0.9999999999999999\n",
      "PC1     0.325234\n",
      "PC2     0.085104\n",
      "PC3     0.077676\n",
      "PC4     0.068997\n",
      "PC5     0.044542\n",
      "PC6     0.033578\n",
      "PC7     0.024592\n",
      "PC8     0.016536\n",
      "PC9     0.015819\n",
      "PC10    0.015359\n",
      "PC11    0.013926\n",
      "PC12    0.012939\n",
      "PC13    0.009901\n",
      "PC14    0.009748\n",
      "PC15    0.009349\n",
      "PC16    0.009036\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "explained_variance_1 = pcoa_results.eigvals / pcoa_results.eigvals.sum()\n",
    "\n",
    "cumulative_explained_variance_1 = explained_variance_1.sum()\n",
    "print(explained_variance_1)\n",
    "print(cumulative_explained_variance_1)\n",
    "print(explained_variance_1.head(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c056ddb-d192-4ad3-bfc7-f02cb2aeb07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from skbio.stats.distance import mantel\n",
    "\n",
    "# reduced_distances = pairwise_distances(reduced, metric='euclidean')\n",
    "# original_distances = pairwise_distances(num_data, metric='euclidean')\n",
    "# correlation, p_value, _ = mantel(original_distances, reduced_distances)\n",
    "# print(f\"Mantel test correlation: {correlation}, p-value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1d1f1a1-8f6d-4e8b-b684-b4eb2685cff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_scaled = pd.DataFrame(scaled)\n",
    "\n",
    "# # dm = pairwise_distances(num_data, metric='euclidean')\n",
    "# # dm = pairwise_distances(scaled_1, metric='euclidean')\n",
    "# # dm = (dm + dm.T) / 2\n",
    "\n",
    "# dm = pairwise_distances(scaled_2, metric='euclidean')\n",
    "\n",
    "\n",
    "# dim = 16\n",
    "\n",
    "# pcoa_results = pcoa(dm)\n",
    "# reduced = pcoa_results.samples.iloc[:, :dim]\n",
    "# print(reduced)\n",
    "\n",
    "\n",
    "# # View the PCoA results\n",
    "# print(pcoa_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c18783d-93bd-44df-b0c5-c36d3b30379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_df(df1, df2):\n",
    "    df1 = pd.DataFrame(df1)\n",
    "    df1 = df1.reset_index(drop=True)\n",
    "    df2 = df2.reset_index(drop=True)\n",
    "    return pd.concat([df1, df2], axis=1)\n",
    "    \n",
    "def get_X_y(reduced, y, cat):\n",
    "    combined_df = combine_df(reduced, y)\n",
    "    cleaned_df = combined_df.dropna()\n",
    "    X = cleaned_df.drop(columns=[cat])\n",
    "    y = cleaned_df[cat]\n",
    "    return X, y\n",
    "\n",
    "def training(X, Y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Logistic Regression\n",
    "    # lr_model = LogisticRegression(random_state=42)\n",
    "    # lr_model.fit(X_train, y_train)\n",
    "    \n",
    "    # y_pred = lr_model.predict(X_test)\n",
    "    # accuracy = accuracy_score(y_test, y_pred)\n",
    "    # print(f\"Logistic Regression Accuracy: {accuracy}\")\n",
    "    # print(f\"Classification Report for Logistic Regression:\\n{classification_report(y_test, y_pred)}\")\n",
    "\n",
    "    # Random Forest Classifier\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_rf = rf_model.predict(X_test)\n",
    "    accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "    print(f\"Random Forest Accuracy: {accuracy_rf}\")\n",
    "    print(f\"Classification Report for Random Forest:\\n{classification_report(y_test, y_pred_rf)}\")\n",
    "\n",
    "    # # MLP Classifier (Neural Network)\n",
    "    # mlp_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=4000, random_state=42)\n",
    "    # mlp_model.fit(X_train, y_train)\n",
    "    \n",
    "    # y_pred_mlp = mlp_model.predict(X_test)\n",
    "    # accuracy_mlp = accuracy_score(y_test, y_pred_mlp)\n",
    "    # print(f\"MLP Classifier Accuracy: {accuracy_mlp}\")\n",
    "    # print(f\"Classification Report for MLP Classifier:\\n{classification_report(y_test, y_pred_mlp)}\")\n",
    "\n",
    "    # XGBoost Classifier\n",
    "    xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_xgb = xgb_model.predict(X_test)\n",
    "    accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "    print(f\"XGBoost Accuracy: {accuracy_xgb}\")\n",
    "    print(f\"Classification Report for XGBoost:\\n{classification_report(y_test, y_pred_xgb)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bfaec82b-1d8d-4fd2-a984-dab901ab3e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat = 'host_age'\n",
    "# cat_y = meta[cat]\n",
    "# cat_X = combine_df(reduced, binary_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "87f7de46-968f-4c73-8a33-06f4a1a1bb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#SampleID\n",
      "qiita_study_id\n",
      "host_age\n",
      "host_age_units\n",
      "host_body_site\n",
      "diagnosis\n",
      "apoe\n"
     ]
    }
   ],
   "source": [
    "for each in meta.columns:\n",
    "    print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e48d564a-d756-4f67-8334-187762df8b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [PC1, PC2, PC3, PC4, PC5, PC6, PC7, PC8, PC9, PC10, PC11, PC12, PC13, PC14, PC15, PC16, PC17, PC18, PC19, PC20, PC21, PC22, PC23, PC24, PC25, PC26, PC27, PC28, PC29, PC30, PC31, PC32, PC33, PC34, PC35, PC36, PC37, PC38, PC39, PC40, PC41, PC42, PC43, PC44, PC45, PC46, PC47, PC48, PC49, PC50, PC51, PC52, PC53, PC54, PC55, PC56, PC57, PC58, PC59, PC60, PC61, PC62, PC63, PC64, PC65, PC66, PC67, PC68, PC69, PC70, PC71, PC72, PC73, PC74, PC75, PC76, PC77, PC78, PC79, PC80, PC81, PC82, PC83, PC84, PC85, PC86, PC87, PC88, PC89, PC90, PC91, PC92, PC93, PC94, PC95, PC96, PC97, PC98, PC99, PC100, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 1697 columns]\n"
     ]
    }
   ],
   "source": [
    "cat = 'apoe'\n",
    "\n",
    "reduced_df = combine_df(reduced, binary_features.T)\n",
    "reduced_df.index = num_data.index\n",
    "reduced_df.index.name = '#SampleID'\n",
    "meta_df = meta.set_index('#SampleID')\n",
    "\n",
    "y_cat = meta_df[cat]\n",
    "binary_data = y_cat.map({'Carrier': 1, 'Non-carrier': 0})\n",
    "y_cat = pd.DataFrame({cat: binary_data})\n",
    "merged_df = pd.merge(reduced_df, y_cat, left_index=True, right_index=True, how='inner')\n",
    "cleaned_df = merged_df.dropna(subset=[cat])\n",
    "f_cleaned_df = merged_df.dropna()\n",
    "\n",
    "print(f_cleaned_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e588d1-84d3-40e0-b02e-5c6a17c9250d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "716a4e03-160c-47ce-857b-fce8e8acee2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1697)\n",
      "(0, 1697)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m X \u001b[38;5;241m=\u001b[39m cleaned_df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[cat])\n\u001b[1;32m     27\u001b[0m Y \u001b[38;5;241m=\u001b[39m cleaned_df[cat]\n\u001b[0;32m---> 28\u001b[0m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[36], line 15\u001b[0m, in \u001b[0;36mtraining\u001b[0;34m(X, Y)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining\u001b[39m(X, Y):\n\u001b[0;32m---> 15\u001b[0m     X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Logistic Regression\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# lr_model = LogisticRegression(random_state=42)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# lr_model.fit(X_train, y_train)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# Random Forest Classifier\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     rf_model \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2778\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2775\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[1;32m   2777\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m-> 2778\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[1;32m   2780\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2782\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m   2783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2408\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2405\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[1;32m   2407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2408\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2409\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2411\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[1;32m   2412\u001b[0m     )\n\u001b[1;32m   2414\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "cat = 'apoe'\n",
    "\n",
    "reduced_df = combine_df(reduced, binary_features.T)\n",
    "reduced_df.index = num_data.index\n",
    "reduced_df.index.name = '#SampleID'\n",
    "meta_df = meta.set_index('#SampleID')\n",
    "\n",
    "y_cat = meta_df[cat]\n",
    "binary_data = y_cat.map({'Carrier': 1, 'Non-carrier': 0})\n",
    "y_cat = pd.DataFrame({cat: binary_data})\n",
    "\n",
    "merged_df = pd.merge(reduced_df, y_cat, left_index=True, right_index=True, how='inner')\n",
    "\n",
    "# print(merged_df.head())\n",
    "# print(merged_df.shape)\n",
    "\n",
    "cleaned_df = merged_df.dropna(subset=[cat])\n",
    "f_cleaned_df = merged_df.dropna()\n",
    "# print(f\"Original dataset shape: {merged_df.shape}\")\n",
    "# print(f\"Cleaned dataset shape: {cleaned_df.shape}\")\n",
    "# Original dataset shape: (1312, 4)\n",
    "# Cleaned dataset shape: (1301, 4)\n",
    "\n",
    "print(cleaned_df.shape)\n",
    "print(f_cleaned_df.shape)\n",
    "X = cleaned_df.drop(columns=[cat])\n",
    "Y = cleaned_df[cat]\n",
    "training(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d986a988-e6c9-48bd-8325-7c58400e6b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059bd592-ffc2-4b3f-999a-68425951460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(binary_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f488b7-5606-4309-b7b4-68738dc386dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCoA, Dim = 16\n",
    "\n",
    "# Linear Regression MSE: 80720753047910.94\n",
    "# Random Forest MSE: 290.40436212971275\n",
    "# XGBoost MSE: 300.0148567682974\n",
    "\n",
    "# PC1     0.325234\n",
    "# PC2     0.085104\n",
    "# PC3     0.077676\n",
    "# PC4     0.068997\n",
    "# PC5     0.044542\n",
    "# PC6     0.033578\n",
    "# PC7     0.024592\n",
    "# PC8     0.016536\n",
    "# PC9     0.015819\n",
    "# PC10    0.015359\n",
    "# PC11    0.013926\n",
    "# PC12    0.012939\n",
    "# PC13    0.009901\n",
    "# PC14    0.009748\n",
    "# PC15    0.009349\n",
    "# PC16    0.009036\n",
    "# dtype: float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a11b5bf-3f6d-4d3e-a9db-3a327dad28c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1727329770.3884413\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "print(start_time)\n",
    "\n",
    "reduced_data = rpca_fr(scaled, numerical_features, 128)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde79c9c-af59-4250-84a5-0b1766e242d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_df(df1, df2):\n",
    "    df1 = pd.DataFrame(df1)\n",
    "    df1 = df1.reset_index(drop=True)\n",
    "    df2 = df2.reset_index(drop=True)\n",
    "    return pd.concat([df1, df2], axis=1)\n",
    "    \n",
    "def get_X_y(reduced, y, cat):\n",
    "    combined_df = combine_df(reduced, y)\n",
    "    cleaned_df = combined_df.dropna()\n",
    "    X = cleaned_df.drop(columns=[cat])\n",
    "    y = cleaned_df[cat]\n",
    "    return X, y\n",
    "\n",
    "def training(X, Y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Linear Regression\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = lr_model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"Linear Regression MSE: {mse}\")\n",
    "\n",
    "    # Random Forest\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_rf = rf_model.predict(X_test)\n",
    "    mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "    print(f\"Random Forest MSE: {mse_rf}\")\n",
    "\n",
    "    # # MLP\n",
    "    # mlp_model = MLPRegressor(hidden_layer_sizes=(100,), max_iter=5000, random_state=42)\n",
    "    # mlp_model.fit(X_train, y_train)\n",
    "    \n",
    "    # y_pred_mlp = mlp_model.predict(X_test)\n",
    "    # mse_mlp = mean_squared_error(y_test, y_pred_mlp)\n",
    "    # print(f\"MLP Regressor MSE: {mse_mlp}\")\n",
    "\n",
    "    # XGBoost\n",
    "    xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_xgb = xgb_model.predict(X_test)\n",
    "    mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "    print(f\"XGBoost MSE: {mse_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8df780-5ec3-4231-938a-7c88524391be",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = 'host_age'\n",
    "cat_y = meta[y_category]\n",
    "cat_X = combine_df(reduced_data, binary_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5c99d1-db7f-4408-bf17-99a49acaf95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_X_y(cat_X, cat_y, cat)\n",
    "training(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b668ea23-b4ab-46b9-a46c-ffd38e5aadb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dim = 3\n",
    "# Linear Regression MSE: 3.995264211009359e+28\n",
    "# Random Forest MSE: 332.3066680347287\n",
    "# /opt/conda/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:697: UserWarning: Training interrupted by user.\n",
    "#   warnings.warn(\"Training interrupted by user.\")\n",
    "# MLP Regressor MSE: 685.7311446346571\n",
    "# XGBoost MSE: 299.14464191813653\n",
    "\n",
    "# Dim = 16\n",
    "# Reconstruction MSE: 0.00022048968863458537\n",
    "# Sample scores shape: (13436, 16)\n",
    "# Feature scores shape: (13795, 16)\n",
    "# Original scaled data shape: (13436, 13795)\n",
    "# Reconstructed data shape: (13436, 13795)\n",
    "\n",
    "# Linear Regression MSE: 1.1185584052698574e+29\n",
    "# Random Forest MSE: 297.4191591508802\n",
    "# XGBoost MSE: 306.4656906982071\n",
    "\n",
    "print(y.max())\n",
    "print(y.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae9e768-2e1c-496a-9c9e-5eda92437a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_d = numerical_features.copy()\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaled_d[numerical_features.columns] = scaler.fit_transform(numerical_features)\n",
    "scaled = pd.DataFrame(scaler.fit_transform(numerical_features.T).T, index=numerical_features.index, columns=numerical_features.columns)\n",
    "\n",
    "# print(\"Scaled Numerical Dataset:\")\n",
    "# print(scaled.head())\n",
    "# print(scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55ea9de-eee2-46e2-8522-0aecb950e111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_feature(row):\n",
    "    unique_values = row.unique()\n",
    "    if len(unique_values) == 2 and set(unique_values).issubset({0, 1}):\n",
    "        return 'binary'\n",
    "    else:\n",
    "        return 'numerical'\n",
    "\n",
    "def split_feature(biom_df):\n",
    "    biom_df['feature_type'] = biom_df.apply(classify_feature, axis=1)\n",
    "\n",
    "    binary_features = biom_df[biom_df['feature_type'] == 'binary']\n",
    "    numerical_features = biom_df[biom_df['feature_type'] == 'numerical']\n",
    "    \n",
    "    binary_features = binary_features.drop(columns=['feature_type'])\n",
    "    numerical_features = numerical_features.drop(columns=['feature_type'])\n",
    "    \n",
    "    print(f\"Number of binary features: {binary_features.shape[0]}\")\n",
    "    print(f\"Number of numerical features: {numerical_features.shape[0]}\")\n",
    "\n",
    "    return binary_features, numerical_features\n",
    "\n",
    "def scale_data(numerical_features):\n",
    "    scaled_d = numerical_features.copy()\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_d[numerical_features.columns] = scaler.fit_transform(numerical_features)\n",
    "    scaled = scaler.fit_transform(numerical_features.T).T\n",
    "    return scaled\n",
    "\n",
    "def rpca_fr(scaled, numerical_features, dim):\n",
    "    scaled += 1e-10\n",
    "    sample_ids = numerical_features.columns.tolist()  # Sample IDs (columns)\n",
    "    feature_ids = numerical_features.index.tolist()   # Feature IDs (rows)\n",
    "    table_scaled = Table(scaled, feature_ids, sample_ids)\n",
    "\n",
    "    rpca_results = rpca(table_scaled, n_components=dim)\n",
    "\n",
    "    ordination, distance = rpca_results\n",
    "    sample_scores = ordination.samples  # Scores for samples\n",
    "    feature_scores = ordination.features  # Scores for features\n",
    "    \n",
    "    X_reconstructed = np.dot(sample_scores, feature_scores.T)\n",
    "    \n",
    "    mse = mean_squared_error(scaled.T, X_reconstructed)\n",
    "    print(f\"Reconstruction MSE: {mse}\")\n",
    "    \n",
    "    print(f\"Sample scores shape: {sample_scores.shape}\")\n",
    "    print(f\"Feature scores shape: {feature_scores.shape}\")\n",
    "    print(f\"Original scaled data shape: {scaled.T.shape}\")\n",
    "    print(f\"Reconstructed data shape: {X_reconstructed.shape}\")\n",
    "\n",
    "    return sample_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caadded7-32e8-477e-87c1-30b8afdf82f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540d470e-ff1e-43ae-9df4-409bd005176a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

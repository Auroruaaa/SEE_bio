{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00d9c271-4658-4c32-8def-c078ab3047d0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Obtaining dependency information for pandas from https://files.pythonhosted.org/packages/fc/a5/4d82be566f069d7a9a702dcdf6f9106df0e0b042e738043c0cc7ddd7e3f6/pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (1.26.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Obtaining dependency information for tzdata>=2022.7 from https://files.pythonhosted.org/packages/65/58/f9c9e6be752e9fcb8b6a0ee9fb87e6e7a1f6bcab2cdc73f02bb7ba91ada0/tzdata-2024.1-py2.py3-none-any.whl.metadata\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Installing collected packages: tzdata, pandas\n",
      "Successfully installed pandas-2.2.2 tzdata-2024.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ea5dbd4-1ae2-4ef5-9bd2-756f5f72f635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy\n",
    "import torch.utils\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.utils.data\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from statistics import mean, stdev, variance\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31ba051d-161a-4520-a97b-b7d9c901a82f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nclass MLP_model (nn.Module):\\n    def __init__(self, input_dim, output_dim):\\n        super().__init__()\\n        self.input_fc=nn.Linear(input_dim,50)\\n        self.input_fc2=nn.Linear(50,25)\\n        self.output_fc=nn.Linear(25,output_dim)\\n\\n    def forward(self,x):\\n       batch_size=x.shape[0]\\n       x = x.view(batch_size, -1)\\n       m_1=nn.functional.relu(self.input_fc(x))\\n       m_2=nn.functional.relu(self.input_fc2(m_1))\\n\\n       output=self.output_fc(m_2)\\n       return output '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP_model (nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.input_fc=nn.Linear(input_dim,800)\n",
    "        self.hidden_fc=nn.Linear(800,600)\n",
    "        self.hidden_fc2=nn.Linear(600,400)\n",
    "        self.hidden_fc3=nn.Linear(400,400)\n",
    "        self.hidden_fc4=nn.Linear(400,400)\n",
    "        self.hidden_fc5=nn.Linear(400,400)\n",
    "        self.hidden_fc6=nn.Linear(400,400)\n",
    "        self.hidden_fc7=nn.Linear(400,400)\n",
    "        self.hidden_fc8=nn.Linear(400,400)\n",
    "        self.hidden_fc9=nn.Linear(400,400)\n",
    "        self.hidden_fc10=nn.Linear(400,100)\n",
    "\n",
    "        self.output_fc=nn.Linear(100,output_dim)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self,x):\n",
    "       batch_size=x.shape[0]\n",
    "       x = x.view(batch_size, -1)\n",
    "       x=self.dropout(x)\n",
    "       m_1=nn.functional.relu(self.input_fc(x))\n",
    "       m_2=nn.functional.relu(self.hidden_fc(m_1))\n",
    "       m_3=nn.functional.relu(self.hidden_fc2(m_2))\n",
    "       m_4=nn.functional.relu(self.hidden_fc3(m_3))\n",
    "       m_4=nn.functional.relu(self.hidden_fc4(m_4))\n",
    "       m_4=nn.functional.relu(self.hidden_fc5(m_4))\n",
    "       m_4=nn.functional.relu(self.hidden_fc6(m_4))\n",
    "       m_4=nn.functional.relu(self.hidden_fc7(m_4))\n",
    "       m_4=nn.functional.relu(self.hidden_fc8(m_4))\n",
    "       m_4=nn.functional.relu(self.hidden_fc9(m_4))\n",
    "       m_4=nn.functional.relu(self.hidden_fc10(m_4))\n",
    "       m_4=self.dropout(m_4)\n",
    "\n",
    "       #output=nn.functional.sigmoid(self.output_fc(m_4))\n",
    "       output=(self.output_fc(m_4))\n",
    "\n",
    "       return output \n",
    "    \n",
    "\"\"\" \n",
    "class MLP_model (nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.input_fc=nn.Linear(input_dim,50)\n",
    "        self.input_fc2=nn.Linear(50,25)\n",
    "        self.output_fc=nn.Linear(25,output_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "       batch_size=x.shape[0]\n",
    "       x = x.view(batch_size, -1)\n",
    "       m_1=nn.functional.relu(self.input_fc(x))\n",
    "       m_2=nn.functional.relu(self.input_fc2(m_1))\n",
    "\n",
    "       output=self.output_fc(m_2)\n",
    "       return output \"\"\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea6bceb7-0e9d-4918-a35b-86303bf76670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred, y):\n",
    "  \n",
    "    top_pred = y_pred.argmax(1)\n",
    "    top_y= y.argmax(1)\n",
    "    correct = top_pred.eq(top_y).sum()\n",
    "    #print(top_pred)\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    #print(correct)\n",
    "    return acc\n",
    "\n",
    "def model_train(model,dataloader,optimizer,citerion,device):\n",
    "    epoch_loss=0\n",
    "    epoch_acc=0\n",
    "    model.train()\n",
    "\n",
    "    for i,data in enumerate(dataloader):\n",
    "        feature,label=data\n",
    "        feature=feature.to(device)\n",
    "        label=label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred=model(feature)\n",
    "        loss=citerion(y_pred,label)\n",
    "        acc=calculate_accuracy(y_pred,label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss+=loss.item()\n",
    "        epoch_acc+=acc.item()\n",
    "\n",
    "    return epoch_loss/len(dataloader),epoch_acc/len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df9ea39a-d6ae-4ffc-a277-3cf21dfdf819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluate(model,dataloader,criterion,device):\n",
    "    epoch_loss=0\n",
    "    epoch_acc=0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i,data in enumerate(dataloader):\n",
    "            feature,label=data\n",
    "            feature=feature.to(device)\n",
    "            label=label.to(device)\n",
    "\n",
    "            y_pred=model(feature)\n",
    "\n",
    "            loss=criterion(y_pred,label)\n",
    "            acc=calculate_accuracy(y_pred,label)\n",
    "\n",
    "            epoch_loss+=loss.item()\n",
    "            epoch_acc+=acc.item()\n",
    "    return epoch_loss/len(dataloader),epoch_acc/len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "229c6cec-bef6-4ddf-a2e3-792d9cf262ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA A100-SXM4-80GB\n",
      "2.2.2\n",
      "1.3.2\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(pd.__version__)\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e6e30a4-08d4-4e1e-b56c-e6d840af60f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_dataset(Dataset):\n",
    "    def __init__(self,csv_file_feature,csv_meta,training_target):\n",
    "        inputdata=pd.read_csv(csv_file_feature)\n",
    "        # Standardize the original data\n",
    "        input_mean, input_std, input_var = inputdata.mean(), inputdata.std(), inputdata.var()\n",
    "        new_std = 1\n",
    "        # print(\"MEAN: \\n\", input_mean)\n",
    "        # print(\"StandardDeviation: \\n\", input_std)\n",
    "        # print(\"Variance: \\n\", input_var)\n",
    "        self.base_frame=(inputdata-input_mean)/(input_std/new_std)\n",
    "        # self.base_frame=inputdata\n",
    "        self.meta_frame=pd.read_csv(csv_meta,dtype={'sample_name':'int32',\n",
    "                      'apoe4':'string',\n",
    "                      'bmi':'float32',\n",
    "                      'diagnosis':'string',\n",
    "                      'amlyoid_positive':'float32',\n",
    "                      'visit':'int32'})\n",
    "        self.target_frame=self.meta_frame[training_target]\n",
    "        self.base_frame.drop(columns=['sample_name'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_frame)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        label=0\n",
    "        if self.target_frame[index]=='positive':\n",
    "            label=[1,0]\n",
    "        else:\n",
    "            label=[0,1]\n",
    "        \n",
    "        feature=self.base_frame.iloc[index]\n",
    "        feature_tensor=torch.tensor(feature.values).float()\n",
    "\n",
    "        label_tensor=torch.tensor(label).float()\n",
    "\n",
    "        return feature_tensor,label_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0ad7074-369b-41c7-b815-c039bc93f09d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data=train_dataset('train_data.csv','Meta.csv','apoe4')\n",
    "train_size= int(0.70*len(train_data))\n",
    "test_size=len(train_data)-train_size\n",
    "\n",
    "training_data,testing_data=torch.utils.data.random_split(train_data,[train_size,test_size])\n",
    "\n",
    "# for ele in training_data:\n",
    "#     print(ele)\n",
    "# mean = torch.mean(training_data)\n",
    "# mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35a715bd-4f20-43f6-8fc7-d42a582680fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t E 0 Train Loss: 0.696 | Train Acc: 37.50%\n",
      "\t E 0 Test. Loss: 0.691 |  Test. Acc: 62.50%\n",
      "\t E 1 Train Loss: 0.689 | Train Acc: 68.18%\n",
      "\t E 1 Test. Loss: 0.690 |  Test. Acc: 58.33%\n",
      "\t E 2 Train Loss: 0.683 | Train Acc: 69.32%\n",
      "\t E 2 Test. Loss: 0.691 |  Test. Acc: 54.17%\n",
      "\t E 3 Train Loss: 0.672 | Train Acc: 69.32%\n",
      "\t E 3 Test. Loss: 0.690 |  Test. Acc: 54.17%\n",
      "\t E 4 Train Loss: 0.625 | Train Acc: 69.32%\n",
      "\t E 4 Test. Loss: 0.724 |  Test. Acc: 54.17%\n",
      "\t E 5 Train Loss: 0.559 | Train Acc: 69.32%\n",
      "\t E 5 Test. Loss: 0.761 |  Test. Acc: 58.33%\n",
      "\t E 6 Train Loss: 0.607 | Train Acc: 61.36%\n",
      "\t E 6 Test. Loss: 0.730 |  Test. Acc: 54.17%\n",
      "\t E 7 Train Loss: 0.597 | Train Acc: 61.36%\n",
      "\t E 7 Test. Loss: 0.672 |  Test. Acc: 58.33%\n",
      "\t E 8 Train Loss: 0.496 | Train Acc: 69.32%\n",
      "\t E 8 Test. Loss: 0.712 |  Test. Acc: 58.33%\n",
      "\t E 9 Train Loss: 0.425 | Train Acc: 69.32%\n",
      "\t E 9 Test. Loss: 0.873 |  Test. Acc: 58.33%\n",
      "\t E 10 Train Loss: 0.332 | Train Acc: 69.32%\n",
      "\t E 10 Test. Loss: 1.065 |  Test. Acc: 62.50%\n",
      "\t E 11 Train Loss: 0.349 | Train Acc: 61.36%\n",
      "\t E 11 Test. Loss: 1.088 |  Test. Acc: 54.17%\n",
      "\t E 12 Train Loss: 0.294 | Train Acc: 75.00%\n",
      "\t E 12 Test. Loss: 1.206 |  Test. Acc: 58.33%\n",
      "\t E 13 Train Loss: 0.307 | Train Acc: 90.91%\n",
      "\t E 13 Test. Loss: 1.234 |  Test. Acc: 63.33%\n",
      "\t E 14 Train Loss: 0.309 | Train Acc: 94.32%\n",
      "\t E 14 Test. Loss: 1.020 |  Test. Acc: 65.00%\n",
      "\t E 15 Train Loss: 0.221 | Train Acc: 94.32%\n",
      "\t E 15 Test. Loss: 1.219 |  Test. Acc: 65.00%\n",
      "\t E 16 Train Loss: 0.111 | Train Acc: 97.73%\n",
      "\t E 16 Test. Loss: 2.398 |  Test. Acc: 51.67%\n",
      "\t E 17 Train Loss: 0.381 | Train Acc: 87.50%\n",
      "\t E 17 Test. Loss: 2.897 |  Test. Acc: 51.67%\n",
      "\t E 18 Train Loss: 0.164 | Train Acc: 95.45%\n",
      "\t E 18 Test. Loss: 0.987 |  Test. Acc: 67.50%\n",
      "\t E 19 Train Loss: 0.116 | Train Acc: 98.86%\n",
      "\t E 19 Test. Loss: 1.132 |  Test. Acc: 56.67%\n",
      "\t E 20 Train Loss: 0.072 | Train Acc: 98.86%\n",
      "\t E 20 Test. Loss: 1.700 |  Test. Acc: 60.83%\n",
      "\t E 21 Train Loss: 0.056 | Train Acc: 98.86%\n",
      "\t E 21 Test. Loss: 2.273 |  Test. Acc: 59.17%\n",
      "\t E 22 Train Loss: 0.034 | Train Acc: 98.86%\n",
      "\t E 22 Test. Loss: 2.274 |  Test. Acc: 65.83%\n",
      "\t E 23 Train Loss: 0.003 | Train Acc: 100.00%\n",
      "\t E 23 Test. Loss: 2.422 |  Test. Acc: 60.83%\n",
      "\t E 24 Train Loss: 0.005 | Train Acc: 100.00%\n",
      "\t E 24 Test. Loss: 2.666 |  Test. Acc: 63.33%\n",
      "\t E 25 Train Loss: 0.004 | Train Acc: 100.00%\n",
      "\t E 25 Test. Loss: 3.071 |  Test. Acc: 60.83%\n",
      "\t E 26 Train Loss: 0.028 | Train Acc: 98.86%\n",
      "\t E 26 Test. Loss: 3.497 |  Test. Acc: 65.00%\n",
      "\t E 27 Train Loss: 0.284 | Train Acc: 95.45%\n",
      "\t E 27 Test. Loss: 1.685 |  Test. Acc: 58.33%\n",
      "\t E 28 Train Loss: 0.179 | Train Acc: 94.32%\n",
      "\t E 28 Test. Loss: 1.107 |  Test. Acc: 60.83%\n",
      "\t E 29 Train Loss: 0.095 | Train Acc: 95.45%\n",
      "\t E 29 Test. Loss: 1.396 |  Test. Acc: 59.17%\n",
      "\t E 30 Train Loss: 0.102 | Train Acc: 95.45%\n",
      "\t E 30 Test. Loss: 1.245 |  Test. Acc: 60.83%\n",
      "\t E 31 Train Loss: 0.051 | Train Acc: 97.73%\n",
      "\t E 31 Test. Loss: 1.913 |  Test. Acc: 65.83%\n",
      "\t E 32 Train Loss: 0.018 | Train Acc: 100.00%\n",
      "\t E 32 Test. Loss: 1.964 |  Test. Acc: 65.83%\n",
      "\t E 33 Train Loss: 0.009 | Train Acc: 100.00%\n",
      "\t E 33 Test. Loss: 2.539 |  Test. Acc: 65.83%\n",
      "\t E 34 Train Loss: 0.003 | Train Acc: 100.00%\n",
      "\t E 34 Test. Loss: 3.268 |  Test. Acc: 61.67%\n",
      "\t E 35 Train Loss: 0.004 | Train Acc: 100.00%\n",
      "\t E 35 Test. Loss: 3.445 |  Test. Acc: 65.83%\n",
      "\t E 36 Train Loss: 0.001 | Train Acc: 100.00%\n",
      "\t E 36 Test. Loss: 3.658 |  Test. Acc: 65.83%\n",
      "\t E 37 Train Loss: 0.003 | Train Acc: 100.00%\n",
      "\t E 37 Test. Loss: 4.157 |  Test. Acc: 63.33%\n",
      "\t E 38 Train Loss: 0.036 | Train Acc: 98.86%\n",
      "\t E 38 Test. Loss: 4.663 |  Test. Acc: 60.83%\n",
      "\t E 39 Train Loss: 0.098 | Train Acc: 96.59%\n",
      "\t E 39 Test. Loss: 2.788 |  Test. Acc: 59.17%\n",
      "\t E 40 Train Loss: 0.106 | Train Acc: 97.73%\n",
      "\t E 40 Test. Loss: 2.470 |  Test. Acc: 61.67%\n",
      "\t E 41 Train Loss: 0.081 | Train Acc: 96.59%\n",
      "\t E 41 Test. Loss: 1.882 |  Test. Acc: 62.50%\n",
      "\t E 42 Train Loss: 0.044 | Train Acc: 98.86%\n",
      "\t E 42 Test. Loss: 1.387 |  Test. Acc: 67.50%\n",
      "\t E 43 Train Loss: 0.020 | Train Acc: 100.00%\n",
      "\t E 43 Test. Loss: 2.109 |  Test. Acc: 55.00%\n",
      "\t E 44 Train Loss: 0.039 | Train Acc: 98.86%\n",
      "\t E 44 Test. Loss: 2.792 |  Test. Acc: 59.17%\n",
      "\t E 45 Train Loss: 0.010 | Train Acc: 100.00%\n",
      "\t E 45 Test. Loss: 2.202 |  Test. Acc: 67.50%\n",
      "\t E 46 Train Loss: 0.003 | Train Acc: 100.00%\n",
      "\t E 46 Test. Loss: 2.851 |  Test. Acc: 59.17%\n",
      "\t E 47 Train Loss: 0.002 | Train Acc: 100.00%\n",
      "\t E 47 Test. Loss: 3.032 |  Test. Acc: 63.33%\n",
      "\t E 48 Train Loss: 0.035 | Train Acc: 98.86%\n",
      "\t E 48 Test. Loss: 3.659 |  Test. Acc: 54.17%\n",
      "\t E 49 Train Loss: 0.022 | Train Acc: 98.86%\n",
      "\t E 49 Test. Loss: 3.053 |  Test. Acc: 63.33%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "lr = 0.0001\n",
    "EPOCHS = 50\n",
    "n_splits = 5  # Number of folds\n",
    "\n",
    "\n",
    "\n",
    "model=MLP_model(883,2) \n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "training_dataloader=DataLoader(training_data,batch_size=8,shuffle=True)\n",
    "testing_dataloader=DataLoader(testing_data,batch_size=8,shuffle=True)\n",
    "\n",
    "# training_features = [data[0] for data in training_data]\n",
    "# training_labels = [data[1]for data in training_data]\n",
    "# training_feature_dataloader = DataLoader(training_features,batch_size=8,shuffle=True)\n",
    "# training_label_dataloader = DataLoader(training_labels,batch_size=8,shuffle=True)\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "\n",
    "    train_loss,train_acc=model_train(model,training_dataloader,optimizer,criterion,device)\n",
    "    # scores = cross_val_score(model, training_feature_dataloader, training_label_dataloader, cv=kf)\n",
    "    test_loss,test_acc=model_evaluate(model,testing_dataloader,criterion,device)\n",
    "    print(f'\\t E {epoch} Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t E {epoch} Test. Loss: {test_loss:.3f} |  Test. Acc: {test_acc*100:.2f}%')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f862633c-ede3-459d-9ebe-b9b5608a482c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8d1d91-9dd8-4222-b72b-e81f5106e6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --normalize--: not really helpful with the original classifier\n",
    "# Cross validation\n",
    "# improvement options (low dep-high dim)\n",
    "# transformer model (pytorch?)\n",
    "# encoding methods\n",
    "\n",
    "# 2217"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5993ef-97ee-4f58-b7d8-b5a5149f9d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "bio summer meeting: 6.19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa975133-64ca-45bb-beea-f3f53227112b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

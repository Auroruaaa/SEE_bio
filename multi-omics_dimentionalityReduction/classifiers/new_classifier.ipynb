{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b514fb9d-ea83-4b08-b1a7-86232a9c9dc3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import torch.utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.data\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "from copy import deepcopy\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "d7002fbb-4ead-4ee3-a7c5-d3cce9cb0c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_dataset(Dataset):\n",
    "    def __init__(self, csv_file_feature, csv_meta, training_target):\n",
    "        self.base_frame = pd.read_csv(csv_file_feature)\n",
    "\n",
    "        self.meta_frame = pd.read_csv(csv_meta, dtype={'sample_name': 'int32',\n",
    "                                                       'apoe4': 'string',\n",
    "                                                       'bmi': 'float32',\n",
    "                                                       'diagnosis': 'string',\n",
    "                                                       'amlyoid_positive': 'float32',\n",
    "                                                       'visit': 'int32'})\n",
    "        self.target_frame = self.meta_frame[training_target]\n",
    "        self.base_frame.drop(columns=['sample_name'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_frame)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = 0\n",
    "        if self.target_frame[index] == 'positive':\n",
    "            label = 1 # [1, 0]\n",
    "        else:\n",
    "            label = 0 # [0, 1]\n",
    "\n",
    "        feature = self.base_frame.iloc[index]\n",
    "        feature_tensor = torch.tensor(feature.values).float()\n",
    "\n",
    "        label_tensor = torch.tensor(label).float()\n",
    "\n",
    "        # return feature_tensor, label_tensor\n",
    "        return feature, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "6dcbf04b-e391-4ae8-9adf-7d191b38a7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(base_matrix):\n",
    "\treturn np.where(base_matrix < 0, -1, 1)\n",
    "\n",
    "# def encoding_rp(X_data, base_matrix, signed=False):\n",
    "#     enc_hvs = []\n",
    "#     # print(len(X_data))\n",
    "#     for i in range(len(X_data)):\n",
    "#         if i % int(len(X_data)/20) == 0:\n",
    "#             sys.stdout.write(str(int(i/len(X_data)*100)) + '% ')\n",
    "#             sys.stdout.flush()\n",
    "#         hv = np.matmul(base_matrix, X_data[i])\n",
    "#         if signed:\n",
    "#             hv = binarize(hv)\n",
    "#         enc_hvs.append(hv)\n",
    "#     return enc_hvs\n",
    "\n",
    "def encoding_rp(X_data, base_matrix, signed=False):\n",
    "    enc_hvs = []\n",
    "    step = max(1, int(len(X_data) / 20))\n",
    "    for i in range(len(X_data)):\n",
    "        if i % step == 0:\n",
    "            sys.stdout.write(str(int(i / len(X_data) * 100)) + '% ')\n",
    "            sys.stdout.flush()\n",
    "        # print(f\"\\nDebug: base_matrix shape: {base_matrix.shape}, X_data[{i}] shape: {X_data[i].shape}\")\n",
    "        hv = np.matmul(base_matrix, X_data[i])\n",
    "        if signed:\n",
    "            hv = binarize(hv)\n",
    "        enc_hvs.append(hv)\n",
    "    return enc_hvs\n",
    "\n",
    "\n",
    "def encoding_idlv(X_data, lvl_hvs, id_hvs, D, bin_len, x_min, L=64):\n",
    "    enc_hvs = []\n",
    "    step = max(1, int(len(X_data) / 20))\n",
    "    for i in range(len(X_data)):\n",
    "        if i == int(len(X_data)/1):\n",
    "            break\n",
    "            \n",
    "        if i % step == 0:\n",
    "            sys.stdout.write(str(int(i/len(X_data)*100)) + '% ')\n",
    "            sys.stdout.flush()\n",
    "        sum_ = np.array([0] * D)\n",
    "        for j in range(len(X_data[i])):\n",
    "            # bin_ = min( np.round((X_data[i][j] - x_min)/bin_len), L-1)\n",
    "            bin_ = min( np.floor((X_data[i][j] - x_min)/bin_len), L-1)\n",
    "            bin_ = int(bin_)\n",
    "            sum_ += lvl_hvs[bin_]*id_hvs[j]\n",
    "        enc_hvs.append(sum_)\n",
    "    return enc_hvs\n",
    "\n",
    "def encoding_perm(X_data, lvl_hvs, D, bin_len, x_min, L=64):\n",
    "    enc_hvs = []\n",
    "    step = max(1, int(len(X_data) / 20))\n",
    "    for i in range(len(X_data)):\n",
    "        if i % step == 0:\n",
    "            sys.stdout.write(str(int(i/len(X_data)*100)) + '% ')\n",
    "            sys.stdout.flush()\n",
    "        sum_ = np.array([0] * D)\n",
    "        for j in range(len(X_data[i])):\n",
    "            # bin_ = min( np.round((X_data[i][j] - x_min)/bin_len), L-1)\n",
    "            bin_ = min( np.floor((X_data[i][j] - x_min)/bin_len), L-1)\n",
    "            bin_ = int(bin_)\n",
    "            sum_ += np.roll(lvl_hvs[bin_], j)\n",
    "        enc_hvs.append(sum_)\n",
    "    return enc_hvs\n",
    "\n",
    "def max_match(class_hvs, enc_hv, class_norms):\n",
    "\t\tmax_score = -np.inf\n",
    "\t\tmax_index = -1\n",
    "\t\tfor i in range(len(class_hvs)):\n",
    "\t\t\tscore = np.matmul(class_hvs[i], enc_hv) / class_norms[i]\n",
    "\t\t\t#score = np.matmul(class_hvs[i], enc_hv)\n",
    "\t\t\tif score > max_score:\n",
    "\t\t\t\tmax_score = score\n",
    "\t\t\t\tmax_index = i\n",
    "\t\treturn max_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "a96b7d00-6bb3-4420-8d09-d336aa5c5d54",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(training_data, testing_data, device, D=500, alg='idlv', epoch=20, lr=1.0, L=64):\n",
    "    training_features = [data[0] for data in training_data]\n",
    "    training_labels = [data[1]for data in training_data]\n",
    "    # print(\"labels: \", training_labels)    \n",
    "    # training_labels = training_labels.tolist()\n",
    "    # print(\"labels: \", training_labels)  \n",
    "    \n",
    "    testing_features = [data[0] for data in testing_data]\n",
    "    testing_labels = [data[1] for data in testing_data]\n",
    "    len_train_f = len(training_features)\n",
    "    cnt_vld = int(0.2 * len_train_f)\n",
    "    \n",
    "    validation_features = training_features[0:cnt_vld]\n",
    "    validation_labels = training_labels[0:cnt_vld]\n",
    "    training_features = training_features[cnt_vld:len_train_f-1]\n",
    "    training_labels = training_labels[cnt_vld:len_train_f-1]\n",
    "    # i = 0\n",
    "    # for eacel in training_features:\n",
    "    #     print(\"feature \", i, \" : \", eacel)\n",
    "    #     i+=1\n",
    "    if alg in ['rp', 'rp-sign']:\n",
    "        #create base matrix\n",
    "        base_matrix = np.random.rand(D, len(training_features[0]))\n",
    "        base_matrix = np.where(base_matrix > 0.5, 1, -1)\n",
    "        base_matrix = np.array(base_matrix, np.int8)\n",
    "        print('\\nEncoding ' + str(len_train_f) + ' train data features')\n",
    "        train_enc_hvs = encoding_rp(training_features, base_matrix, signed=(alg == 'rp-sign'))\n",
    "        print('\\n\\nEncoding ' + str(len(validation_features)) + ' features validation data')\n",
    "        validation_enc_hvs = encoding_rp(validation_features, base_matrix, signed=(alg == 'rp-sign'))\n",
    "    elif alg in ['idlv', 'perm']:\n",
    "        #create level matrix\n",
    "        lvl_hvs = []\n",
    "        temp = [-1]*int(D/2) + [1]*int(D/2)\n",
    "        np.random.shuffle(temp)\n",
    "        lvl_hvs.append(temp)\n",
    "        change_list = np.arange(0, D)\n",
    "        np.random.shuffle(change_list)\n",
    "        cnt_toChange = int(D/2 / (L-1))\n",
    "        for i in range(1, L):\n",
    "            temp = np.array(lvl_hvs[i-1])\n",
    "            temp[change_list[(i-1)*cnt_toChange : i*cnt_toChange]] = -temp[change_list[(i-1)*cnt_toChange : i*cnt_toChange]]\n",
    "            lvl_hvs.append(list(temp))\n",
    "        lvl_hvs = np.array(lvl_hvs, dtype=np.int8)\n",
    "        x_min = min( np.min(training_features), np.min(validation_features) )\n",
    "        x_max = max( np.max(training_features), np.max(validation_features) )\n",
    "        bin_len = (x_max - x_min)/float(L)\n",
    "        \n",
    "        #need to create id hypervectors if encoding is level-id\n",
    "        if alg == 'idlv':\n",
    "            cnt_id = len(training_features[0])\n",
    "            id_hvs = []\n",
    "            for i in range(cnt_id):\n",
    "                temp = [-1]*int(D/2) + [1]*int(D/2)\n",
    "                np.random.shuffle(temp)\n",
    "                id_hvs.append(temp)\n",
    "            id_hvs = np.array(id_hvs, dtype=np.int8)\n",
    "            print('\\nEncoding ' + str(len_train_f) + ' train data features')\n",
    "            train_enc_hvs = encoding_idlv(training_features, lvl_hvs, id_hvs, D, bin_len, x_min, L)\n",
    "            print('\\n\\nEncoding ' + str(len(validation_features)) + ' features validation data')\n",
    "            validation_enc_hvs = encoding_idlv(validation_features, lvl_hvs, id_hvs, D, bin_len, x_min, L)\n",
    "        elif alg == 'perm':\n",
    "            print('\\nEncoding ' + str(len_train_f) + ' train data features')\n",
    "            train_enc_hvs = encoding_perm(training_features, lvl_hvs, D, bin_len, x_min, L)\n",
    "            print('\\n\\nEncoding ' + str(len(validation_features)) + ' features validation data')\n",
    "            validation_enc_hvs = encoding_perm(validation_features, lvl_hvs, D, bin_len, x_min, L)\n",
    "\t\n",
    "    #training, initial model\n",
    "    class_hvs = [[0.] * D] * (max(training_labels) + 1)\n",
    "    for i in range(len(train_enc_hvs)):\n",
    "        class_hvs[training_labels[i]] += train_enc_hvs[i]\n",
    "    class_norms = [np.linalg.norm(hv) for hv in class_hvs]\n",
    "    class_hvs_best = deepcopy(class_hvs)\n",
    "    class_norms_best = deepcopy(class_norms)\n",
    "    #retraining\n",
    "    if epoch > 0:\n",
    "        acc_max = -np.inf\n",
    "        print('\\n\\n' + str(epoch) + ' retraining epochs')\n",
    "        for i in range(epoch):\n",
    "            sys.stdout.write('epoch ' + str(i) + ': ')\n",
    "            sys.stdout.flush()\n",
    "            #shuffle data during retraining\n",
    "            pickList = np.arange(0, len(train_enc_hvs))\n",
    "            np.random.shuffle(pickList)\n",
    "            for j in pickList:\n",
    "                predict = max_match(class_hvs, train_enc_hvs[j], class_norms)\n",
    "                if predict != training_labels[j]:\n",
    "                    class_hvs[predict] -= np.multiply(lr, train_enc_hvs[j])\n",
    "                    class_hvs[training_labels[j]] += np.multiply(lr, train_enc_hvs[j])\n",
    "            class_norms = [np.linalg.norm(hv) for hv in class_hvs]\n",
    "            correct = 0\n",
    "            for j in range(len(validation_enc_hvs)):\n",
    "                predict = max_match(class_hvs, validation_enc_hvs[j], class_norms)\n",
    "                if predict == validation_labels[j]:\n",
    "                    correct += 1\n",
    "            acc = float(correct)/len(validation_enc_hvs)\n",
    "            sys.stdout.write(\"%.4f \\n\" %acc) \n",
    "            sys.stdout.flush()\n",
    "            if i > 0 and i%5 == 0:\n",
    "                print('')\n",
    "            if acc > acc_max:\n",
    "                acc_max = acc\n",
    "                class_hvs_best = deepcopy(class_hvs)\n",
    "                class_norms_best = deepcopy(class_norms)\n",
    "   \n",
    "    del training_features\n",
    "    del validation_features\n",
    "    del train_enc_hvs\n",
    "    del validation_enc_hvs \n",
    "    \n",
    "    print('\\n\\nEncoding ' + str(len(testing_features)) + ' test data features')\n",
    "    if alg == 'rp' or alg == 'rp-sign':\n",
    "        test_enc_hvs = encoding_rp(testing_features, base_matrix, signed=(alg == 'rp-sign'))\n",
    "    elif alg == 'idlv':\n",
    "        test_enc_hvs = encoding_idlv(testing_features, lvl_hvs, id_hvs, D, bin_len, x_min, L)\n",
    "    elif alg == 'perm':\n",
    "            test_enc_hvs = encoding_perm(testing_features, lvl_hvs, D, bin_len, x_min, L)\n",
    "    correct = 0\n",
    "    for i in range(len(test_enc_hvs)):\n",
    "        predict = max_match(class_hvs_best, test_enc_hvs[i], class_norms_best)\n",
    "        if predict == testing_labels[i]:\n",
    "            correct += 1\n",
    "    acc = float(correct)/len(test_enc_hvs)\n",
    "    return acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "4f4fe99b-0007-411d-97fd-a3d212d4fec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA A100-SXM4-80GB\n",
      "2.2.2\n",
      "1.3.2\n",
      "\n",
      "Encoding 81 train data features\n",
      "0% 4% 9% 14% 18% 23% 28% 32% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_260/1580132966.py:45: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  bin_ = min( np.floor((X_data[i][j] - x_min)/bin_len), L-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37% 42% 46% 51% 56% 60% 65% 70% 75% 79% 84% 89% 93% 98% \n",
      "\n",
      "Encoding 16 features validation data\n",
      "0% 6% 12% 18% 25% 31% 37% 43% 50% 56% 62% 68% 75% 81% 87% 93% \n",
      "\n",
      "20 retraining epochs\n",
      "epoch 0: 0.5625 \n",
      "epoch 1: 0.5625 \n",
      "epoch 2: 0.6250 \n",
      "epoch 3: 0.6250 \n",
      "epoch 4: 0.6250 \n",
      "epoch 5: 0.6250 \n",
      "\n",
      "epoch 6: 0.6875 \n",
      "epoch 7: 0.6875 \n",
      "epoch 8: 0.6875 \n",
      "epoch 9: 0.6875 \n",
      "epoch 10: 0.6875 \n",
      "\n",
      "epoch 11: 0.6875 \n",
      "epoch 12: 0.6875 \n",
      "epoch 13: 0.6875 \n",
      "epoch 14: 0.6875 \n",
      "epoch 15: 0.6875 \n",
      "\n",
      "epoch 16: 0.6875 \n",
      "epoch 17: 0.6875 \n",
      "epoch 18: 0.6875 \n",
      "epoch 19: 0.6875 \n",
      "\n",
      "\n",
      "Encoding 35 test data features\n",
      "0% 2% 5% 8% 11% 14% 17% 20% 22% 25% 28% 31% 34% 37% 40% 42% 45% 48% 51% 54% 57% 60% 62% 65% 68% 71% 74% 77% 80% 82% 85% 88% 91% 94% 97% "
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5714285714285714"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(pd.__version__)\n",
    "print(sklearn.__version__)\n",
    "\n",
    "train_data = train_dataset('train_data.csv', 'Meta.csv', 'apoe4')\n",
    "\n",
    "\n",
    "train_size = int(0.70 * len(train_data))\n",
    "test_size = len(train_data) - train_size\n",
    "\n",
    "training_data, testing_data = torch.utils.data.random_split(train_data, [train_size, test_size])\n",
    "training_dataloader = DataLoader(training_data, batch_size=8, shuffle=True)\n",
    "testing_dataloader = DataLoader(testing_data, batch_size=8, shuffle=True)\n",
    "# for el in training_dataloader:\n",
    "#     print(el)\n",
    "\n",
    "train_model(training_data, testing_data, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21e67cfd-2414-4b16-8240-29107af29502",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# re:\n",
    "# training: 0.62~0.75\n",
    "# testing: 0.6~0.628\n",
    "# idlv:\n",
    "# training: 0.5...~0.68\n",
    "# testing: 0.6\n",
    "# perm: \n",
    "# training eventually stuck at 0.6875~0.7500\n",
    "# testing: always 0.54~0.71\n",
    "\n",
    "# --------\n",
    "# /tmp/ipykernel_260/1580132966.py:61: \n",
    "# FutureWarning: \n",
    "# Series.__getitem__ treating keys as positions is deprecated. \n",
    "# In a future version, integer keys will always be treated as labels \n",
    "# (consistent with DataFrame behavior). \n",
    "# To access a value by position, use `ser.iloc[pos]`\n",
    "\n",
    "#   bin_ = min( np.floor((X_data[i][j] - x_min)/bin_len), L-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29c51609-b08d-43e8-9fde-3a540861b1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ele in training_dataloader:\n",
    "#     print(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ff93a3-af93-48f5-9451-39d11ddfdb8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7ff569d-bb11-45ea-a9ef-ad2e1cce9c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from biom import Table\n",
    "from gemelli.rpca import rpca\n",
    "# from gemelli.factorization import rpca\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "606456fe-433d-4807-8cb2-0c885d0bc35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_binary_columns(X_train):\n",
    "    binary_columns = []\n",
    "    for col in range(X_train.shape[1]):\n",
    "        unique_values = np.unique(X_train[:, col])\n",
    "        if set(unique_values).issubset({0, 1}):\n",
    "            binary_columns.append(col)\n",
    "    return binary_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae753757-3f38-4d88-8817-ea4021c9984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"dataset_amyloid.pickle\"\n",
    "with open(dataset_path, 'rb') as f:\n",
    "    X_train, y_train, X_test, y_test = pickle.load(f)\n",
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "Y = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "binary_columns = find_binary_columns(X)\n",
    "\n",
    "numerical_columns = []\n",
    "for i in range(0,10193):\n",
    "    if i not in binary_columns:\n",
    "        numerical_columns.append(i)\n",
    "\n",
    "X_bi = X[:, binary_columns] # Binary dataset\n",
    "X_nu = X[:, numerical_columns] # Numerical dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ca844b9-a935-4821-a8e8-c367bcff84e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Binary columns = (190, 3887)\n",
      "Number of Numerical columns = (190, 6306)\n",
      "Minimum value of numerical element = 0.0\n",
      "Maximum value of numerical element = 5347441.0\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of Binary columns = {X_bi.shape}')\n",
    "print(f'Number of Numerical columns = {X_nu.shape}')\n",
    "print(f'Minimum value of numerical element = {np.min(X_nu)}')\n",
    "print(f'Maximum value of numerical element = {np.max(X_nu)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0abf1299-d317-4ace-8796-36b7ee350d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum value of scaled numerical element = -1.22930590661059\n",
      "Maximum value of scaled numerical element = 13.74772708486753\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_nu)\n",
    "\n",
    "print(f'Minimum value of scaled numerical element = {np.min(X_scaled)}')\n",
    "print(f'Maximum value of scaled numerical element = {np.max(X_scaled)}')\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "# Y_numerical = label_encoder.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6640f50-b894-4265-b4f6-8989bace8f06",
   "metadata": {},
   "outputs": [
    {
     "ename": "TableException",
     "evalue": "Duplicate observation IDs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTableException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m sample_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, X_scaled\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# Sample IDs will be 1, 2, 3, ..., 190\u001b[39;00m\n\u001b[1;32m      2\u001b[0m feature_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, X_scaled\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# Feature IDs will be 1, 2, 3, ..., 6000\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m bt \u001b[38;5;241m=\u001b[39m \u001b[43mTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m ordination, distance_matrix \u001b[38;5;241m=\u001b[39m rpca(bt, n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m) \n\u001b[1;32m      6\u001b[0m reduced_features \u001b[38;5;241m=\u001b[39m ordination\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/biom/table.py:522\u001b[0m, in \u001b[0;36mTable.__init__\u001b[0;34m(self, data, observation_ids, sample_ids, observation_metadata, sample_metadata, table_id, type, create_date, generated_by, observation_group_metadata, sample_group_metadata, validate, observation_index, sample_index, **kwargs)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_observation_group_metadata \u001b[38;5;241m=\u001b[39m observation_group_metadata\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate:\n\u001b[0;32m--> 522\u001b[0m     \u001b[43merrcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;66;03m# These will be set by _index_ids()\u001b[39;00m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/biom/err.py:474\u001b[0m, in \u001b[0;36merrcheck\u001b[0;34m(table, *errtypes)\u001b[0m\n\u001b[1;32m    472\u001b[0m ret \u001b[38;5;241m=\u001b[39m __errprof\u001b[38;5;241m.\u001b[39mtest(table, \u001b[38;5;241m*\u001b[39merrtypes)\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, \u001b[38;5;167;01mException\u001b[39;00m):\n\u001b[0;32m--> 474\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ret\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mTableException\u001b[0m: Duplicate observation IDs"
     ]
    }
   ],
   "source": [
    "sample_ids = list(range(1, X_scaled.shape[0] + 1))  # Sample IDs will be 1, 2, 3, ..., 190\n",
    "feature_ids = list(range(1, X_scaled.shape[1] + 1))  # Feature IDs will be 1, 2, 3, ..., 6000\n",
    "bt = Table(X_scaled, feature_ids, sample_ids)\n",
    "\n",
    "ordination, distance_matrix = rpca(bt, n_components=128) \n",
    "reduced_features = ordination.samples\n",
    "reduced_features_df = pd.DataFrame(reduced_features, index=sample_ids)\n",
    "\n",
    "print(reduced_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c011110-fdf4-4b12-a882-b56a1233b3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # 1. Handling Infinite Values\n",
    "# print(\"Checking for infinite values...\")\n",
    "# if np.isinf(X_scaled).any():\n",
    "#     print(\"Infinite values found. Replacing with the maximum finite value.\")\n",
    "#     X_scaled[np.isinf(X_scaled)] = np.finfo(np.float64).max\n",
    "# else:\n",
    "#     print(\"No infinite values found.\")\n",
    "\n",
    "# # 2. Handling NaN Values\n",
    "# print(\"\\nChecking for NaN values...\")\n",
    "# if np.isnan(X_scaled).any():\n",
    "#     print(\"NaN values found. Replacing with the mean of the column.\")\n",
    "#     # Calculate column means\n",
    "#     col_means = np.nanmean(X_scaled, axis=0)\n",
    "#     # Find indices where NaN values are present\n",
    "#     inds = np.where(np.isnan(X_scaled))\n",
    "#     # Replace NaNs with the corresponding column mean\n",
    "#     X_scaled[inds] = np.take(col_means, inds[1])\n",
    "# else:\n",
    "#     print(\"No NaN values found.\")\n",
    "\n",
    "# # 3. Handling Duplicate Rows/Columns\n",
    "# print(\"\\nChecking for duplicate rows and columns...\")\n",
    "\n",
    "# # Check for duplicate rows\n",
    "# unique_rows = np.unique(X_scaled, axis=0)\n",
    "# if unique_rows.shape[0] != X_scaled.shape[0]:\n",
    "#     print(f\"Duplicate rows found. Removing {X_scaled.shape[0] - unique_rows.shape[0]} duplicate rows.\")\n",
    "#     X_scaled = unique_rows\n",
    "# else:\n",
    "#     print(\"No duplicate rows found.\")\n",
    "\n",
    "# # Check for duplicate columns\n",
    "# unique_columns = np.unique(X_scaled, axis=1)\n",
    "# if unique_columns.shape[1] != X_scaled.shape[1]:\n",
    "#     print(f\"Duplicate columns found. Removing {X_scaled.shape[1] - unique_columns.shape[1]} duplicate columns.\")\n",
    "#     X_scaled = unique_columns\n",
    "# else:\n",
    "#     print(\"No duplicate columns found.\")\n",
    "\n",
    "# print(\"\\nData cleaning completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ac5bf6-4c72-41bf-869f-c590a95b92a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_zero_rows = np.all(X_scaled == 0, axis=1)\n",
    "# all_zero_columns = np.all(X_scaled == 0, axis=0)\n",
    "\n",
    "# print(f\"All-zero rows: {all_zero_rows.sum()}\")\n",
    "# print(f\"All-zero columns: {all_zero_columns.sum()}\")\n",
    "\n",
    "# # # Option 2: Replace all-zero rows/columns with a small constant (e.g., 1e-6)\n",
    "# small_constant = 1e-6  # You can adjust this value if needed\n",
    "\n",
    "# if all_zero_rows.sum() > 0:\n",
    "#     print(f\"Replacing {all_zero_rows.sum()} all-zero rows with {small_constant}.\")\n",
    "#     X_scaled[all_zero_rows, :] = small_constant\n",
    "\n",
    "# if all_zero_columns.sum() > 0:\n",
    "#     print(f\"Replacing {all_zero_columns.sum()} all-zero columns with {small_constant}.\")\n",
    "#     X_scaled[:, all_zero_columns] = small_constant\n",
    "\n",
    "# Create numerical sample and feature IDs\n",
    "sample_ids = list(range(1, X_scaled.shape[0] + 1))  # Sample IDs will be 1, 2, 3, ..., 190\n",
    "feature_ids = list(range(1, X_scaled.shape[1] + 1))  # Feature IDs will be 1, 2, 3, ..., 6306\n",
    "\n",
    "# Convert the numpy array into a biom.Table\n",
    "bt = Table(X_scaled, feature_ids, sample_ids)\n",
    "\n",
    "# Run RPCA to reduce the features\n",
    "ordination, distance_matrix = rpca(bt, n_components=128)  # Reduce to 128 components\n",
    "\n",
    "# Extract the reduced features (principal components)\n",
    "reduced_features = ordination.samples\n",
    "\n",
    "# Convert reduced features to a pandas DataFrame for easier handling\n",
    "reduced_features_df = pd.DataFrame(reduced_features, index=sample_ids)\n",
    "\n",
    "# Print the reduced features\n",
    "print(reduced_features_df)\n",
    "\n",
    "# If you want to save the reduced features to a CSV file:\n",
    "reduced_features_df.to_csv('reduced_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169574dd-2379-412a-8d27-8c8f93bc3566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_ids = list(range(1, X_scaled.shape[0] + 1))  # Sample IDs will be 1, 2, 3, ..., 190\n",
    "# feature_ids = list(range(1, X_scaled.shape[1] + 1))  # Feature IDs will be 1, 2, 3, ..., 6306\n",
    "\n",
    "# # Debugging: Print the shapes and IDs to verify\n",
    "# print(f\"Shape of X_scaled: {X_scaled.shape}\")\n",
    "# print(f\"Number of sample IDs: {len(sample_ids)}\")\n",
    "# print(f\"Number of feature IDs: {len(feature_ids)}\")\n",
    "\n",
    "# # Check if the IDs are unique\n",
    "# assert len(sample_ids) == len(set(sample_ids)), \"Sample IDs are not unique!\"\n",
    "# assert len(feature_ids) == len(set(feature_ids)), \"Feature IDs are not unique!\"\n",
    "\n",
    "# # Convert the numpy array into a biom.Table\n",
    "# bt = Table(X_scaled, feature_ids, sample_ids)\n",
    "\n",
    "# # Run RPCA to reduce the features\n",
    "# ordination, distance_matrix = rpca(bt, n_components=128)  # Here, we reduce to 128 components\n",
    "\n",
    "# # Extract the reduced features (principal components)\n",
    "# reduced_features = ordination.samples\n",
    "\n",
    "# # Convert reduced features to a pandas DataFrame for easier handling\n",
    "# reduced_features_df = pd.DataFrame(reduced_features, index=sample_ids)\n",
    "\n",
    "# # Print the reduced features\n",
    "# print(reduced_features_df)\n",
    "\n",
    "# # If you want to save the reduced features to a CSV file:\n",
    "# reduced_features_df.to_csv('reduced_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc933697-7e82-4ce4-8e97-7f38a0eb25e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

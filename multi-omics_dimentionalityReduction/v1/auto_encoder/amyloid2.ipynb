{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f63d55ad-f918-43c4-96f1-e8db3a8b5073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "# from torchvision import datasets, transforms\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "527e7f80-0868-4279-bc97-aa13eb248d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def get_interpolations(args, model, device, images, images_per_row=20):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        def interpolate(t1, t2, num_interps):\n",
    "            alpha = np.linspace(0, 1, num_interps+2)\n",
    "            interps = []\n",
    "            for a in alpha:\n",
    "                interps.append(a*t2.view(1, -1) + (1 - a)*t1.view(1, -1))\n",
    "            return torch.cat(interps, 0)\n",
    "\n",
    "        if args.model == 'VAE':\n",
    "            mu, logvar = model.encode(images.view(-1, 784))\n",
    "            embeddings = model.reparameterize(mu, logvar).cpu()\n",
    "        elif args.model == 'AE':\n",
    "            embeddings = model.encode(images.view(-1, 784))\n",
    "            \n",
    "        interps = []\n",
    "        for i in range(0, images_per_row+1, 1):\n",
    "            interp = interpolate(embeddings[i], embeddings[i+1], images_per_row-4)\n",
    "            interp = interp.to(device)\n",
    "            interp_dec = model.decode(interp)\n",
    "            line = torch.cat((images[i].view(-1, 784), interp_dec, images[i+1].view(-1, 784)))\n",
    "            interps.append(line)\n",
    "        # Complete the loop and append the first image again\n",
    "        interp = interpolate(embeddings[i+1], embeddings[0], images_per_row-4)\n",
    "        interp = interp.to(device)\n",
    "        interp_dec = model.decode(interp)\n",
    "        line = torch.cat((images[i+1].view(-1, 784), interp_dec, images[0].view(-1, 784)))\n",
    "        interps.append(line)\n",
    "\n",
    "        interps = torch.cat(interps, 0).to(device)\n",
    "    return interps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a34a1739-6212-4d9d-836c-8d6ba7ec8158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture\n",
    "class FC_Encoder(nn.Module):\n",
    "    def __init__(self, output_size):\n",
    "        super(FC_Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return h1\n",
    "\n",
    "class FC_Decoder(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super(FC_Decoder, self).__init__()\n",
    "        self.fc3 = nn.Linear(embedding_size, 1024)\n",
    "        self.fc4 = nn.Linear(1024, 784)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "class CNN_Encoder(nn.Module):\n",
    "    def __init__(self, output_size, input_size=(1, 155, 10193)):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.channel_mult = 16\n",
    "\n",
    "        #convolutions\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,\n",
    "                     out_channels=self.channel_mult*1,\n",
    "                     kernel_size=4,\n",
    "                     stride=1,\n",
    "                     padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(self.channel_mult*1, self.channel_mult*2, 4, 2, 1),\n",
    "            nn.BatchNorm2d(self.channel_mult*2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(self.channel_mult*2, self.channel_mult*4, 4, 2, 1),\n",
    "            nn.BatchNorm2d(self.channel_mult*4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(self.channel_mult*4, self.channel_mult*8, 4, 2, 1),\n",
    "            nn.BatchNorm2d(self.channel_mult*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(self.channel_mult*8, self.channel_mult*16, 3, 2, 1),\n",
    "            nn.BatchNorm2d(self.channel_mult*16),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        self.flat_fts = self.get_flat_fts(self.conv)\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(self.flat_fts, output_size),\n",
    "            nn.BatchNorm1d(output_size),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def get_flat_fts(self, fts):\n",
    "        f = fts(Variable(torch.ones(1, *self.input_size)))\n",
    "        return int(np.prod(f.size()[1:]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x.view(-1, *self.input_size))\n",
    "        x = x.view(-1, self.flat_fts)\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     batch_size = x.size(0)\n",
    "    #     x = self.conv(x.view(batch_size, -1))\n",
    "    #     x = x.view(-1, self.flat_fts)\n",
    "    #     return self.linear(x)\n",
    "        \n",
    "class CNN_Decoder(nn.Module):\n",
    "    def __init__(self, embedding_size, input_size=(1,155, 10193)):\n",
    "        super(CNN_Decoder, self).__init__()\n",
    "        self.input_height = 28\n",
    "        self.input_width = 28\n",
    "        self.input_dim = embedding_size\n",
    "        self.channel_mult = 16\n",
    "        self.output_channels = 1\n",
    "        self.fc_output_dim = 512\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, self.fc_output_dim),\n",
    "            nn.BatchNorm1d(self.fc_output_dim),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.deconv = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(self.fc_output_dim, self.channel_mult*4,\n",
    "                                4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(self.channel_mult*4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. self.channel_mult*32 x 4 x 4\n",
    "            nn.ConvTranspose2d(self.channel_mult*4, self.channel_mult*2,\n",
    "                                3, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.channel_mult*2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. self.channel_mult*16 x 7 x 7\n",
    "            nn.ConvTranspose2d(self.channel_mult*2, self.channel_mult*1,\n",
    "                                4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.channel_mult*1),\n",
    "            nn.ReLU(True),\n",
    "            # state size. self.channel_mult*8 x 14 x 14\n",
    "            nn.ConvTranspose2d(self.channel_mult*1, self.output_channels, 4, 2, 1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "            # state size. self.output_channels x 28 x 28\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, self.fc_output_dim, 1, 1)\n",
    "        x = self.deconv(x)\n",
    "        return x.view(-1, self.input_width*self.input_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6cab98b-5b40-429f-9be5-2ac92fe435b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Network, self).__init__()\n",
    "        output_size = args.embedding_size\n",
    "        self.encoder = CNN_Encoder(output_size)\n",
    "\n",
    "        self.decoder = CNN_Decoder(args.embedding_size)\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure the input tensor has the correct number of elements\n",
    "        batch_size = x.size(0)\n",
    "        z = self.encode(x.view(batch_size, -1))  # Reshape to (batch_size, 784)\n",
    "        return self.decode(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d95ad42-e27f-42e6-a7f7-ec7573230140",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(object):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.device = \"cuda\"\n",
    "        self._init_dataset()\n",
    "        self.train_loader = self.X_train_loader\n",
    "        self.test_loader = self.X_test_loader\n",
    "\n",
    "        self.model = Network(args)\n",
    "        self.model.to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "\n",
    "    def _init_dataset(self):\n",
    "        dataset_path = self.args.dataset\n",
    "        with open(dataset_path, 'rb') as f:\n",
    "            self.X_train, self.y_train, self.X_test, self.y_test = pickle.load(f)\n",
    "        self.X_train = torch.tensor(self.X_train, dtype=torch.float32)\n",
    "        self.y_train = torch.tensor(self.y_train, dtype=torch.float32)\n",
    "        self.X_test = torch.tensor(self.X_test, dtype=torch.float32)\n",
    "        self.y_test = torch.tensor(self.y_test, dtype=torch.float32)\n",
    "        \n",
    "        train_dataset = torch.utils.data.TensorDataset(self.X_train, self.y_train)\n",
    "        test_dataset = torch.utils.data.TensorDataset(self.X_test, self.y_test)\n",
    "        \n",
    "        self.X_train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=self.args.batch_size, shuffle=True)\n",
    "        self.X_test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=self.args.batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "    def loss_function(self, recon_x, x):\n",
    "        BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "        return BCE\n",
    "\n",
    "    def train(self, epoch):\n",
    "        self.model.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx, (data, _) in enumerate(self.train_loader):\n",
    "            data = data.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            recon_batch = self.model(data)\n",
    "            loss = self.loss_function(recon_batch, data)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            self.optimizer.step()\n",
    "            if batch_idx % self.args.log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(self.train_loader.dataset),\n",
    "                    100. * batch_idx / len(self.train_loader),\n",
    "                    loss.item() / len(data)))\n",
    "\n",
    "        print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "              epoch, train_loss / len(self.train_loader.dataset)))\n",
    "\n",
    "    def test(self, epoch):\n",
    "        self.model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for i, (data, _) in enumerate(self.test_loader):\n",
    "                data = data.to(self.device)\n",
    "                recon_batch = self.model(data)\n",
    "                test_loss += self.loss_function(recon_batch, data).item()\n",
    "\n",
    "        test_loss /= len(self.test_loader.dataset)\n",
    "        print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d44173b7-41c8-45d7-9a4b-3ad007d582ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 128\n",
    "# epochs = 10\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# seed = 42\n",
    "# log_interval = 10\n",
    "# embedding_size = 32\n",
    "# results_path = 'results/'\n",
    "# model_type = 'AE'\n",
    "\n",
    "# batch_size = args['batch_size']\n",
    "# epochs = args['epochs']\n",
    "# cuda = args['no-cuda']\n",
    "# seed = args['seed']\n",
    "# log_interval = args['log_interval']\n",
    "# embedding_size = args['embedding_size']\n",
    "# results_path = args['results_path']\n",
    "# model_type = args['model']\n",
    "# dataset_name = args['dataset']\n",
    "args = {\n",
    "    'batch_size': 128,\n",
    "    'epochs': 10,\n",
    "    'no_cuda': False,\n",
    "    'seed': 42,\n",
    "    'log_interval': 10,\n",
    "    'embedding_size': 32,\n",
    "    'results_path': 'results/',\n",
    "    'model': 'AE',\n",
    "    'dataset': \"dataset_amyloid.pickle\",\n",
    "    # 'X_train': X_train,\n",
    "    # 'y_train': y_train,\n",
    "    # 'X_test': X_test,\n",
    "    # 'y_test': y_test,\n",
    "}\n",
    "\n",
    "class Args:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)\n",
    "\n",
    "args = Args(**args)\n",
    "\n",
    "\n",
    "batch_size = args.batch_size\n",
    "epochs = args.epochs\n",
    "use_cuda = not args.no_cuda\n",
    "seed = args.seed\n",
    "log_interval = args.log_interval\n",
    "embedding_size = args.embedding_size\n",
    "results_path = args.results_path\n",
    "model_type = args.model\n",
    "dataset_name = args.dataset\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if use_cuda and torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "autoenc = AE(args)\n",
    "\n",
    "# dataset_path = \"dataset_amyloid.pickle\"\n",
    "# with open(dataset_path, 'rb') as f:\n",
    "#     X_train, y_train, X_test, y_test = pickle.load(f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc112da3-272c-40b0-b95c-b48c71ca7500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "311fb272-fcc6-4a12-bc5b-695613c16a3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 1, 155, 10193]' is invalid for input of size 1304704",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, args\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 9\u001b[0m         \u001b[43mautoenc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m         autoenc\u001b[38;5;241m.\u001b[39mtest(epoch)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mSystemExit\u001b[39;00m):\n",
      "Cell \u001b[0;32mIn[24], line 39\u001b[0m, in \u001b[0;36mAE.train\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     37\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 39\u001b[0m recon_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_function(recon_batch, data)\n\u001b[1;32m     41\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[23], line 18\u001b[0m, in \u001b[0;36mNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Ensure the input tensor has the correct number of elements\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Reshape to (batch_size, 784)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(z)\n",
      "Cell \u001b[0;32mIn[23], line 10\u001b[0m, in \u001b[0;36mNetwork.encode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[22], line 63\u001b[0m, in \u001b[0;36mCNN_Encoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 63\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_size\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     64\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflat_fts)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(x)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 1, 155, 10193]' is invalid for input of size 1304704"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        os.stat(args.results_path)\n",
    "    except :\n",
    "        os.mkdir(args.results_path)\n",
    "\n",
    "    try:\n",
    "        for epoch in range(1, args.epochs + 1):\n",
    "            autoenc.train(epoch)\n",
    "            autoenc.test(epoch)\n",
    "    except (KeyboardInterrupt, SystemExit):\n",
    "        print(\"Manual Interruption\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa16488-a16f-4027-b0b1-53f064409180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60415507-db56-46a0-98bc-622e355012f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "262158a4-5c50-4fbb-8c90-c0428e60001d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-15 21:20:34.059226: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-15 21:20:34.076653: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-15 21:20:34.081984: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-15 21:20:34.096167: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-15 21:20:34.892705: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils \n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.utils.data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.datasets import mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e40e7e0-fd3d-4af8-abf5-8bfabb5d963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the Autoencoder model as a subclass of the TensorFlow Model class\n",
    "\n",
    "class SimpleAutoencoder(Model):\n",
    "\tdef __init__(self,latent_dimensions , data_shape):\n",
    "\t\tsuper(SimpleAutoencoder, self).__init__()\n",
    "\t\tself.latent_dimensions = latent_dimensions\n",
    "\t\tself.data_shape = data_shape\n",
    "\n",
    "\t\t# Encoder architecture using a Sequential model\n",
    "\t\tself.encoder = tf.keras.Sequential([\n",
    "\t\t\tlayers.Flatten(),\n",
    "\t\t\tlayers.Dense(latent_dimensions, activation='relu', dtype='float32'),\n",
    "\t\t])\n",
    "\n",
    "\t\t# Decoder architecture using another Sequential model\n",
    "\t\tself.decoder = tf.keras.Sequential([\n",
    "\t\t\tlayers.Dense(tf.math.reduce_prod(data_shape), activation='sigmoid', dtype='float32'),\n",
    "\t\t\tlayers.Reshape(data_shape)\n",
    "\t\t])\n",
    "\n",
    "\t# Forward pass method defining the encoding and decoding steps\n",
    "\tdef call(self, input_data):\n",
    "\t\tencoded_data = self.encoder(input_data)\n",
    "\t\tdecoded_data = self.decoder(encoded_data)\n",
    "\t\treturn decoded_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df237b49-fa19-4afc-8e07-6056e30db02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.flatten_layer  =tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(64, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(32, activation=tf.nn.relu)\n",
    "        \n",
    "        \n",
    "        self.bottleneck = tf.keras.layers.Dense(16, activation=tf.nn.relu)\n",
    "    \n",
    "        self.dense4 = tf.keras.layers.Dense(32, activation=tf.nn.relu)\n",
    "        self.dense5 = tf.keras.layers.Dense(64, activation=tf.nn.relu)\n",
    "        \n",
    "        self.dense_final = tf.keras.layers.Dense(784)\n",
    "    \n",
    "    def call(self, inp):\n",
    "        x_reshaped = self.flatten_layer(inp)\n",
    "        #print(x_reshaped.shape)\n",
    "        x = self.dense1(x_reshaped)\n",
    "        x = self.dense2(x)\n",
    "        x = self.bottleneck(x)\n",
    "        x_hid= x\n",
    "        x = self.dense4(x)\n",
    "        x = self.dense5(x)\n",
    "        x = self.dense_final(x)\n",
    "        return x, x_reshaped,x_hid\n",
    "\n",
    "# define loss function and gradient\n",
    "def loss(x, x_bar, h, model, Lambda =100):\n",
    "    reconstruction_loss = tf.reduce_mean( \n",
    "                tf.keras.losses.mse(x, x_bar) \n",
    "            ) \n",
    "    reconstruction_loss *= 28 * 28\n",
    "    W= tf.Variable(model.bottleneck.weights[0])\n",
    "    dh = h * (1 - h)  # N_batch x N_hidden\n",
    "    W = tf.transpose(W)\n",
    "    contractive = Lambda * tf.reduce_sum(tf.linalg.matmul(dh**2,\n",
    "                                                          tf.square(W)),\n",
    "                                         axis=1)\n",
    "    total_loss = reconstruction_loss + contractive\n",
    "    return total_loss\n",
    "\n",
    "def grad(model, inputs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        reconstruction, inputs_reshaped, hidden = model(inputs)\n",
    "        loss_value = loss(inputs_reshaped, reconstruction, hidden, model)\n",
    "    return loss_value, tape.gradient(loss_value, model.trainable_variables), inputs_reshaped, reconstruction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17fd2814-017d-4563-af9f-66cb9c77764d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA A100-SXM4-80GB\n",
      "2.2.2\n",
      "1.3.2\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'SimpleAutoencoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 76\u001b[0m\n\u001b[1;32m     73\u001b[0m input_data_shape \u001b[38;5;241m=\u001b[39m X_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m     74\u001b[0m latent_dimensions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[0;32m---> 76\u001b[0m simple_autoencoder \u001b[38;5;241m=\u001b[39m \u001b[43mSimpleAutoencoder\u001b[49m(latent_dimensions, input_data_shape)\n\u001b[1;32m     77\u001b[0m simple_autoencoder\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39mlosses\u001b[38;5;241m.\u001b[39mMeanSquaredError())\n\u001b[1;32m     78\u001b[0m simple_autoencoder\u001b[38;5;241m.\u001b[39mfit(X_train, y_train,\n\u001b[1;32m     79\u001b[0m \t\t\t\tepochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     80\u001b[0m \t\t\t\tshuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     81\u001b[0m \t\t\t\tvalidation_data\u001b[38;5;241m=\u001b[39m(X_test, y_test))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SimpleAutoencoder' is not defined"
     ]
    }
   ],
   "source": [
    "dataset_path = \"dataset_amyloid.pickle\"\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "print(pd.__version__)\n",
    "print(sklearn.__version__)\n",
    "\n",
    "\n",
    "with open(dataset_path, 'rb') as f:\n",
    "    X_train, y_train, X_test, y_test = pickle.load(f)\n",
    "# X_train = X_train.astype('int32') \n",
    "# X_test = X_test.astype('int32') \n",
    "X_train = X_train.astype('float32') / 255.\n",
    "X_test = X_test.astype('float32') / 255.\n",
    "\n",
    "# train the model\n",
    "# model = AutoEncoder()\n",
    "# optimizer = tf.optimizers.Adam(learning_rate=0.001)\n",
    "# global_step = tf.Variable(0)\n",
    "# num_epochs = 10\n",
    "# batch_size = 32\n",
    "# for epoch in range(num_epochs):\n",
    "#     print(\"Epoch: \", epoch)\n",
    "#     for x in range(0, len(X_train), batch_size):\n",
    "#         x_inp = X_train[x : x + batch_size]\n",
    "#         loss_value, grads, inputs_reshaped, reconstruction = grad(model, x_inp)\n",
    "#         optimizer.apply_gradients(zip(grads, model.trainable_variables),\n",
    "#                               global_step)\n",
    "        \n",
    "#     print(\"Step: {}, Loss: {}\".format(global_step.numpy(),tf.reduce_sum(loss_value)))\n",
    "\n",
    "\n",
    "# # Define the optimizer\n",
    "# optimizer = tf.optimizers.Adam()\n",
    "\n",
    "# # Define the loss function\n",
    "# loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# # Function to calculate gradients\n",
    "# def grad(model, inputs):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         reconstruction = model(inputs)\n",
    "#         # Ensure reconstruction is a tensor, not a tuple\n",
    "#         if isinstance(reconstruction, tuple):\n",
    "#             reconstruction = reconstruction[0]\n",
    "#         print(f\"Inputs shape: {inputs.shape}\")\n",
    "#         print(f\"Reconstruction shape: {reconstruction.shape}\")\n",
    "#         loss_value = loss_fn(inputs, reconstruction)\n",
    "#     grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "#     return loss_value, grads, inputs, reconstruction\n",
    "\n",
    "# global_step = tf.Variable(0)\n",
    "# num_epochs = 200\n",
    "# batch_size = 128\n",
    "\n",
    "# # Ensure X_train is a numpy array with correct dtype\n",
    "# X_train = np.array(X_train, dtype=np.float32)\n",
    "\n",
    "# # Check the shape of the data\n",
    "# print(f\"X_train shape: {X_train.shape}\")\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     print(\"Epoch: \", epoch)\n",
    "#     for x in range(0, len(X_train), batch_size):\n",
    "#         x_inp = X_train[x : x + batch_size]\n",
    "#         loss_value, grads, inputs_reshaped, reconstruction = grad(model, x_inp)\n",
    "#         optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "#     print(\"Step: {}, Loss: {}\".format(global_step.numpy(), tf.reduce_sum(loss_value)))\n",
    "\n",
    "#--------------------\n",
    "input_data_shape = X_test.shape[1:]\n",
    "latent_dimensions = 64\n",
    "\n",
    "simple_autoencoder = SimpleAutoencoder(latent_dimensions, input_data_shape)\n",
    "simple_autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "simple_autoencoder.fit(X_train, y_train,\n",
    "\t\t\t\tepochs=1,\n",
    "\t\t\t\tshuffle=True,\n",
    "\t\t\t\tvalidation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20704c71-4d60-44c0-acab-fc38c3cb5eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(155, 10193)\n",
      "(155,)\n",
      "(35, 10193)\n",
      "(35,)\n",
      "[1 1 1 0 1 1 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 1 1\n",
      " 0 0 0 0 1 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 0 1 1 0\n",
      " 1 0 0 1 0 0 0 1 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0\n",
      " 1 1 0 0 1 1]\n",
      "<class 'numpy.ndarray'>\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "print(y_train[1:])\n",
    "print(type(y_train))\n",
    "print(y_train.dtype)\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd336f4-aeef-49b0-a6b0-7791b71d6c50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0262e65-690a-403c-8d18-14e538a8b673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac922f7f-a048-47bb-953c-782d20b5c923",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

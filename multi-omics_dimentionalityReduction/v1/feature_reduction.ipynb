{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "624b658d-1e57-49c1-977f-bfe9e6d2a32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-15 19:52:23.450920: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-15 19:52:23.467625: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-15 19:52:23.472746: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-15 19:52:23.484986: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-15 19:52:24.202085: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, mean_squared_error\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import SVC\n",
    "# from scipy.stats import ttest_ind, chi2_contingency, pearsonr\n",
    "# from statsmodels.stats.multitest import multipletests\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.layers import Input, Dense, concatenate, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92030c7b-21b0-4369-810a-a1790eba0d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_binary_columns(X_train):\n",
    "    binary_columns = []\n",
    "    for col in range(X_train.shape[1]):\n",
    "        unique_values = np.unique(X_train[:, col])\n",
    "        if set(unique_values).issubset({0, 1}):\n",
    "            binary_columns.append(col)\n",
    "    return binary_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "322c1573-e061-4997-9ffb-5e55c4e30a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"dataset_amyloid.pickle\"\n",
    "with open(dataset_path, 'rb') as f:\n",
    "    X_train, y_train, X_test, y_test = pickle.load(f)\n",
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "Y = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "binary_columns = find_binary_columns(X)\n",
    "\n",
    "numerical_columns = []\n",
    "for i in range(0,10193):\n",
    "    if i not in binary_columns:\n",
    "        numerical_columns.append(i)\n",
    "\n",
    "X_bi = X[:, binary_columns] # Binary dataset\n",
    "X_nu = X[:, numerical_columns] # Numerical dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48779f45-5711-462c-8b1f-08f384f0eae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Binary columns = (190, 3887)\n",
      "Number of Numerical columns = (190, 6306)\n",
      "Minimum value of numerical element = 0.0\n",
      "Maximum value of numerical element = 5347441.0\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of Binary columns = {X_bi.shape}')\n",
    "print(f'Number of Numerical columns = {X_nu.shape}')\n",
    "print(f'Minimum value of numerical element = {np.min(X_nu)}')\n",
    "print(f'Maximum value of numerical element = {np.max(X_nu)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "466e22a3-5337-4fc6-8fd7-a5cb86d49535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum value of scaled numerical element = -1.22930590661059\n",
      "Maximum value of scaled numerical element = 13.74772708486753\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_nu)\n",
    "\n",
    "print(f'Minimum value of scaled numerical element = {np.min(X_scaled)}')\n",
    "print(f'Maximum value of scaled numerical element = {np.max(X_scaled)}')\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "# Y_numerical = label_encoder.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "314b4be5-cc78-46a0-8bd7-0e61510488d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_model(X, n_component):\n",
    "    print(f'Method: PCA')\n",
    "    pca = PCA(n_components=n_component)\n",
    "    X_reduced = pca.fit_transform(X)\n",
    "    X_reconstructed = pca.inverse_transform(X_reduced)\n",
    "    \n",
    "    print(f'Number of components selected: {pca.n_components_}')\n",
    "    mse = mean_squared_error(X.flatten(), X_reconstructed.flatten())\n",
    "    print(f'Mean Squared Error (MSE): {mse}')\n",
    "    \n",
    "    return X_reduced\n",
    "\n",
    "def nmf_model(X, n_component):\n",
    "    print(f'Method: NMF')\n",
    "    nmf = NMF(n_components=n_component, init='random', random_state=0)\n",
    "    X_reduced = nmf.fit_transform(X)\n",
    "    X_reconstructed = nmf.inverse_transform(X_reduced)\n",
    "    \n",
    "    print(f'Number of components selected: {nmf.n_components_}')\n",
    "    mse = mean_squared_error(X.flatten(), X_reconstructed.flatten())\n",
    "    print(f'Mean Squared Error (MSE): {mse}')\n",
    "\n",
    "    return X_reduced\n",
    "\n",
    "def svm_model(X, Y, kernel=\"linear\"):\n",
    "    print(f'Method: SVM')\n",
    "    svm = SVC(kernel=kernel)\n",
    "    svm.fit(X, Y)\n",
    "    \n",
    "    selector = SelectFromModel(svm, prefit=True)\n",
    "    X_reduced = selector.transform(X)\n",
    "\n",
    "    print(\"Number of coponents selected:\", X_reduced.shape[1])\n",
    "\n",
    "    return X_reduced\n",
    "\n",
    "def rf_model(X, Y, threshold, n_estimators=100, random_state=42):\n",
    "    print(f'Method: Random Forest')\n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)\n",
    "    rf.fit(X, Y)\n",
    "    \n",
    "    selector = SelectFromModel(rf, threshold=threshold, prefit=True)\n",
    "    X_reduced = selector.transform(X)\n",
    "    \n",
    "    selected_features_indices = selector.get_support(indices=True)\n",
    "\n",
    "    print(f'Random Forest Threshold: {threshold}')\n",
    "    print(f'Number of coponents selected: {X_reduced.shape[1]}')\n",
    "    # print(f'Selected Features: {selected_features_indices}')\n",
    "    \n",
    "    return X_reduced #, selected_features_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "055570f6-ce8f-4c27-81e0-0390053c7a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Numerical Dataset----\n",
      "Method: PCA\n",
      "Number of components selected: 128\n",
      "Mean Squared Error (MSE): 0.03259457373367942\n",
      "----Binary Dataset----\n",
      "Method: NMF\n",
      "Number of components selected: 64\n",
      "Mean Squared Error (MSE): 0.0011488281093814806\n",
      "Method: SVM\n",
      "Number of coponents selected: 916\n",
      "Method: Random Forest\n",
      "Random Forest Threshold: 0.003\n",
      "Number of coponents selected: 65\n"
     ]
    }
   ],
   "source": [
    "print(f'----Numerical Dataset----')\n",
    "pca_data = pca_model(X_scaled, 128)\n",
    "\n",
    "print(f'----Binary Dataset----')\n",
    "nmf_data = nmf_model(X_bi, 64)\n",
    "svm_data = svm_model(X_bi, Y)\n",
    "rf_threshold = 0.003\n",
    "rf_data = rf_model(X_bi, Y, rf_threshold)\n",
    "\n",
    "# print(pca_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16b2d2bb-914b-4e3a-8df6-60377ad4a978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Dataset: \n",
    "# Method: PCA\n",
    "# Number of components selected: 128\n",
    "# Mean Squared Error (MSE): 0.032486597153881774\n",
    "# Binary Dataset: \n",
    "# Method: NMF\n",
    "# Number of components selected: 64\n",
    "# Mean Squared Error (MSE): 0.0011488281093814806\n",
    "# Method: SVM\n",
    "# Number of coponents selected: 916\n",
    "# Method: Random Forest\n",
    "# Random Forest Threshold: mean\n",
    "# Number of coponents selected: 754"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88e13636-89ad-4adc-bc51-cfc56f850c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def late_fusion_features(X_1, X_2):\n",
    "\n",
    "#     # Define the input shapes based on the data\n",
    "#     input_shape_1 = X_1.shape[1]\n",
    "#     input_shape_2 = X_2.shape[1]\n",
    "\n",
    "#     # Define the first input stream for numerical data\n",
    "#     input_1 = Input(shape=(input_shape_1,), name='input_1')\n",
    "#     dense_1 = Dense(64, activation='relu')(input_1)\n",
    "#     dense_1 = Dense(32, activation='relu')(dense_1)\n",
    "#     output_1 = Dense(16, activation='relu', name='output_1')(dense_1)\n",
    "\n",
    "#     # Define the second input stream for binary data\n",
    "#     input_2 = Input(shape=(input_shape_2,), name='input_2')\n",
    "#     dense_2 = Dense(64, activation='relu')(input_2)\n",
    "#     dense_2 = Dense(32, activation='relu')(dense_2)\n",
    "#     output_2 = Dense(16, activation='relu', name='output_2')(dense_2)\n",
    "\n",
    "#     # Late fusion via concatenation of features\n",
    "#     fused_features = concatenate([output_1, output_2], name='fused_features')\n",
    "\n",
    "#     # Create the model\n",
    "#     model = Model(inputs=[input_1, input_2], outputs=fused_features)\n",
    "\n",
    "#     # Compile the model (necessary to run Keras models)\n",
    "#     model.compile(optimizer='adam', loss='mse')  # Using mse as a placeholder loss\n",
    "\n",
    "#     # Use the model to predict the fused features\n",
    "#     X_fusion = model.predict([X_1, X_2])\n",
    "\n",
    "#     # Clean up Keras session\n",
    "#     K.clear_session()\n",
    "\n",
    "#     return X_fusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3dd1c08-7fd3-4f4e-9dd2-439c52e1fd81",
   "metadata": {},
   "outputs": [],
   "source": [
    " def select_confident_features(x):\n",
    "        k = 8  # number of top features to select from each stream\n",
    "        x_sorted = tf.sort(tf.abs(x), direction='DESCENDING')\n",
    "        x_top_k = x_sorted[:, :k]\n",
    "        return x_top_k\n",
    "     \n",
    "def late_fusion_features(X_1, X_2):\n",
    "\n",
    "    device = '/gpu:0' if tf.config.list_physical_devices('GPU') else '/cpu:0'\n",
    "    print(device)\n",
    "    with tf.device(device):\n",
    "        # Define the input shapes based on the data\n",
    "        input_shape_1 = X_1.shape[1]\n",
    "        input_shape_2 = X_2.shape[1]\n",
    "\n",
    "        # Define the first input stream for the first feature set\n",
    "        input_1 = Input(shape=(input_shape_1,), name='input_1')\n",
    "        dense_1 = Dense(64, activation='relu')(input_1)\n",
    "        dense_1 = Dense(32, activation='relu')(dense_1)\n",
    "        output_1 = Dense(16, activation='relu', name='output_1')(dense_1)\n",
    "\n",
    "        # Define the second input stream for the second feature set\n",
    "        input_2 = Input(shape=(input_shape_2,), name='input_2')\n",
    "        dense_2 = Dense(64, activation='relu')(input_2)\n",
    "        dense_2 = Dense(32, activation='relu')(dense_2)\n",
    "        output_2 = Dense(16, activation='relu', name='output_2')(dense_2)\n",
    "\n",
    "        top_features_1 = Lambda(select_confident_features)(output_1)\n",
    "        top_features_2 = Lambda(select_confident_features)(output_2)\n",
    "\n",
    "        # Late fusion via concatenation of the most confident features\n",
    "        fused_features = concatenate([top_features_1, top_features_2], name='fused_features')\n",
    "    \n",
    "        # Create the model\n",
    "        model = Model(inputs=[input_1, input_2], outputs=fused_features)\n",
    "    \n",
    "        # Use the model to predict the fused features\n",
    "        X_fusion = model.predict([X_1, X_2])\n",
    "\n",
    "    return X_fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7731151-4de6-4671-be47-32b669512371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpu:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-15 19:52:45.782273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79078 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:0b:00.0, compute capability: 8.0\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1723751566.662418    1279 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1723751566.663527    1274 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1723751566.664612    1276 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1723751566.665700    1275 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1723751566.666819    1282 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1723751566.667897    1272 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1723751566.668974    1277 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1723751566.670094    1283 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1723751566.671185    1278 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1723751566.672266    1273 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1723751566.673344    1281 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1723751566.674435    1284 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1723751566.675523    1285 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n"
     ]
    }
   ],
   "source": [
    "X_fusion = late_fusion_features(X_nu, X_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ff6c05-225a-4648-a3ab-db7f9e8479a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e6c1a4-b103-409d-9340-77bd30c2bbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "# from tensorflow.keras.models import Model\n",
    "\n",
    "# # Numerical data input and processing\n",
    "# input_pca = Input(shape=(128,), name='pca_input')\n",
    "# x_pca = Dense(64, activation='relu', name='pca_dense1')(input_pca)\n",
    "# x_pca = Dense(32, activation='relu', name='pca_dense2')(x_pca)\n",
    "\n",
    "# # Binary data input and processing\n",
    "# input_nmf = Input(shape=(64,), name='nmf_input')\n",
    "# x_nmf = Dense(32, activation='relu', name='nmf_dense1')(input_nmf)\n",
    "# x_nmf = Dense(16, activation='relu', name='nmf_dense2')(x_nmf)\n",
    "\n",
    "# # Feature fusion\n",
    "# combined = Concatenate(name='concatenate')([x_pca, x_nmf])\n",
    "# output = Dense(1, activation='sigmoid', name='output')(combined)\n",
    "\n",
    "# # Model definition\n",
    "# model = Model(inputs=[input_pca, input_nmf], outputs=output)\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# model.summary()\n",
    "\n",
    "# # Example data\n",
    "# # import numpy as np\n",
    "# # pca_data = np.random.rand(190, 124)  # Replace with actual pca_data\n",
    "# # nmf_data = np.random.randint(2, size=(190, 64))  # Replace with actual nmf_data\n",
    "# # Y = np.random.randint(2, size=190)  # Replace with actual target data\n",
    "\n",
    "# # Model training\n",
    "# model.fit([pca_data, nmf_data], Y, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed189ada-1804-4f64-83e1-09544e2eb2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ———— New New Version\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultimodalModel, self).__init__()\n",
    "        # Numerical data processing layers\n",
    "        self.pca_dense1 = nn.Linear(128, 64)\n",
    "        self.pca_dense2 = nn.Linear(64, 32)\n",
    "        # Binary data processing layers\n",
    "        self.nmf_dense1 = nn.Linear(64, 32)\n",
    "        self.nmf_dense2 = nn.Linear(32, 16)\n",
    "        # Combined layers\n",
    "        self.combined_dense1 = nn.Linear(32 + 16, 16)\n",
    "        self.combined_dense2 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, pca_input, nmf_input):\n",
    "        # Process numerical data\n",
    "        x_pca = torch.relu(self.pca_dense1(pca_input))\n",
    "        x_pca = torch.relu(self.pca_dense2(x_pca))\n",
    "        # Process binary data\n",
    "        x_nmf = torch.relu(self.nmf_dense1(nmf_input))\n",
    "        x_nmf = torch.relu(self.nmf_dense2(x_nmf))\n",
    "        # Concatenate and process combined features\n",
    "        combined = torch.cat((x_pca, x_nmf), dim=1)\n",
    "        x_combined = torch.relu(self.combined_dense1(combined))\n",
    "        output = torch.sigmoid(self.combined_dense2(x_combined))\n",
    "        return output, combined\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "model = MultimodalModel().to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "pca_torch = torch.tensor(pca_data, dtype=torch.float32).to(device)\n",
    "nmf_torch = torch.tensor(nmf_data, dtype=torch.float32).to(device)\n",
    "Y_torch = torch.tensor(Y, dtype=torch.float32).to(device)\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i in range(0, len(Y), batch_size):\n",
    "        pca_batch = pca_torch[i:i + batch_size]\n",
    "        nmf_batch = nmf_torch[i:i + batch_size]\n",
    "        y_batch = Y_torch[i:i + batch_size]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs, combined_features = model(pca_batch, nmf_batch)\n",
    "        loss = criterion(outputs.squeeze(), y_batch)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(Y)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    final_output, combined_features = model(pca_torch, nmf_torch)\n",
    "    # print(f'Final Output: {final_output}')\n",
    "    # print(f'Combined Features: {combined_features}')\n",
    "print(combined_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0e0a78-7a67-4ce6-9cc0-e42a9260809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fd4206-57bf-4480-bb50-92e4106ae21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from fastFM import sgd\n",
    "\n",
    "# Combine data into a single feature set\n",
    "combined_data = np.hstack((pca_data, nmf_data))\n",
    "\n",
    "# Convert to dictionary format for DictVectorizer\n",
    "data_dict = [dict(enumerate(row)) for row in combined_data]\n",
    "\n",
    "vec = DictVectorizer()\n",
    "X = vec.fit_transform(data_dict)\n",
    "\n",
    "Y = np.random.randint(2, size=190)  # Replace with actual target data\n",
    "\n",
    "# Train Factorization Machine\n",
    "fm = sgd.FMClassification(n_iter=100, init_stdev=0.1, l2_reg_w=0.1, l2_reg_V=0.1, rank=10)\n",
    "fm.fit(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24402e59-628f-4732-98e7-37bfb00db32d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gemelli'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Union, Optional\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskbio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TreeNode, OrdinationResults, DistanceMatrix\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgemelli\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatrix_completion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatrixCompletion\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgemelli\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptspace\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OptSpace\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgemelli\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (matrix_rclr,\n\u001b[1;32m     20\u001b[0m                                    fast_unifrac,\n\u001b[1;32m     21\u001b[0m                                    bp_read_phylogeny,\n\u001b[1;32m     22\u001b[0m                                    retrieve_t2t_taxonomy,\n\u001b[1;32m     23\u001b[0m                                    create_taxonomy_metadata,\n\u001b[1;32m     24\u001b[0m                                    mask_value_only)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gemelli'"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Copyright (c) 2019--, gemelli development team.\n",
    "#\n",
    "# Distributed under the terms of the Modified BSD License.\n",
    "#\n",
    "# The full license is in the file COPYING.txt, distributed with this software.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "import biom\n",
    "import skbio\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial import distance\n",
    "from typing import Union, Optional\n",
    "from skbio import TreeNode, OrdinationResults, DistanceMatrix\n",
    "from gemelli.matrix_completion import MatrixCompletion\n",
    "from gemelli.optspace import OptSpace\n",
    "from gemelli.preprocessing import (matrix_rclr,\n",
    "                                   fast_unifrac,\n",
    "                                   bp_read_phylogeny,\n",
    "                                   retrieve_t2t_taxonomy,\n",
    "                                   create_taxonomy_metadata,\n",
    "                                   mask_value_only)\n",
    "from gemelli._defaults import (DEFAULT_COMP, DEFAULT_MTD,\n",
    "                               DEFAULT_MSC, DEFAULT_MFC,\n",
    "                               DEFAULT_OPTSPACE_ITERATIONS,\n",
    "                               DEFAULT_MFF, DEFAULT_METACV,\n",
    "                               DEFAULT_COLCV, DEFAULT_TESTS,\n",
    "                               DEFAULT_MATCH, DEFAULT_TRNSFRM)\n",
    "from scipy.linalg import svd\n",
    "# import QIIME2 if in a Q2env otherwise set type to str\n",
    "try:\n",
    "    from q2_types.tree import NewickFormat\n",
    "except ImportError:\n",
    "    # python does not check but technically this is the type\n",
    "    NewickFormat = str\n",
    "\n",
    "\n",
    "def phylogenetic_rpca_without_taxonomy(\n",
    "        table: biom.Table,\n",
    "        phylogeny: NewickFormat,\n",
    "        n_components: Union[int, str] = DEFAULT_COMP,\n",
    "        min_sample_count: int = DEFAULT_MSC,\n",
    "        min_feature_count: int = DEFAULT_MFC,\n",
    "        min_feature_frequency: float = DEFAULT_MFF,\n",
    "        min_depth: int = DEFAULT_MTD,\n",
    "        max_iterations: int = DEFAULT_OPTSPACE_ITERATIONS) -> (\n",
    "        OrdinationResults, DistanceMatrix,\n",
    "        TreeNode, biom.Table):\n",
    "    \"\"\"\n",
    "    Runs phylogenetic RPCA without taxonomy. This code will\n",
    "    be run QIIME2 versions of gemelli. Outside of QIIME2\n",
    "    please use phylogenetic_rpca.\n",
    "    \"\"\"\n",
    "\n",
    "    output = phylogenetic_rpca(table=table,\n",
    "                               phylogeny=phylogeny,\n",
    "                               n_components=n_components,\n",
    "                               min_sample_count=min_sample_count,\n",
    "                               min_feature_count=min_feature_count,\n",
    "                               min_feature_frequency=min_feature_frequency,\n",
    "                               min_depth=min_depth,\n",
    "                               max_iterations=max_iterations)\n",
    "    ord_res, dist_res, phylogeny, counts_by_node, _ = output\n",
    "\n",
    "    return ord_res, dist_res, phylogeny, counts_by_node\n",
    "\n",
    "\n",
    "def phylogenetic_rpca_with_taxonomy(\n",
    "            table: biom.Table,\n",
    "            phylogeny: NewickFormat,\n",
    "            taxonomy: pd.DataFrame,\n",
    "            n_components: Union[int, str] = DEFAULT_COMP,\n",
    "            min_sample_count: int = DEFAULT_MSC,\n",
    "            min_feature_count: int = DEFAULT_MFC,\n",
    "            min_feature_frequency: float = DEFAULT_MFF,\n",
    "            min_depth: int = DEFAULT_MTD,\n",
    "            max_iterations: int = DEFAULT_OPTSPACE_ITERATIONS) -> (\n",
    "        OrdinationResults, DistanceMatrix,\n",
    "        TreeNode, biom.Table, pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Runs phylogenetic RPCA with taxonomy. This code will\n",
    "    be run QIIME2 versions of gemelli. Outside of QIIME2\n",
    "    please use phylogenetic_rpca.\n",
    "    \"\"\"\n",
    "\n",
    "    output = phylogenetic_rpca(table=table,\n",
    "                               phylogeny=phylogeny,\n",
    "                               taxonomy=taxonomy,\n",
    "                               n_components=n_components,\n",
    "                               min_sample_count=min_sample_count,\n",
    "                               min_feature_count=min_feature_count,\n",
    "                               min_feature_frequency=min_feature_frequency,\n",
    "                               min_depth=min_depth,\n",
    "                               max_iterations=max_iterations)\n",
    "    ord_res, dist_res, phylogeny, counts_by_node, result_taxonomy = output\n",
    "\n",
    "    return ord_res, dist_res, phylogeny, counts_by_node, result_taxonomy\n",
    "\n",
    "\n",
    "def phylogenetic_rpca(table: biom.Table,\n",
    "                      phylogeny: NewickFormat,\n",
    "                      taxonomy: Optional[pd.DataFrame] = None,\n",
    "                      n_components: Union[int, str] = DEFAULT_COMP,\n",
    "                      min_sample_count: int = DEFAULT_MSC,\n",
    "                      min_feature_count: int = DEFAULT_MFC,\n",
    "                      min_feature_frequency: float = DEFAULT_MFF,\n",
    "                      min_depth: int = DEFAULT_MTD,\n",
    "                      max_iterations: int = DEFAULT_OPTSPACE_ITERATIONS) -> (\n",
    "                          OrdinationResults, DistanceMatrix,\n",
    "                          TreeNode, biom.Table, Optional[pd.DataFrame]):\n",
    "    \"\"\"\n",
    "    Performs robust phylogenetic center log-ratio transform and\n",
    "    robust PCA. The robust PCA and enter log-ratio transform\n",
    "    operate on only observed values of the data.\n",
    "    For more information see (1 and 2).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    table: numpy.ndarray, required\n",
    "    The feature table in biom format containing the\n",
    "    samples over which metric should be computed.\n",
    "\n",
    "    phylogeny: str, required\n",
    "    Path to the file containing the phylogenetic tree containing tip\n",
    "    identifiers that correspond to the feature identifiers in the table.\n",
    "    This tree can contain tip ids that are not present in the table,\n",
    "    but all feature ids in the table must be present in this tree.\n",
    "\n",
    "    taxonomy: pd.DataFrame, optional\n",
    "    Taxonomy file in QIIME2 formatting. See the feature metdata\n",
    "    section of https://docs.qiime2.org/2021.11/tutorials/metadata\n",
    "\n",
    "    n_components: int, optional : Default is 3\n",
    "    The underlying rank of the data and number of\n",
    "    output dimensions.\n",
    "\n",
    "    min_sample_count: int, optional : Default is 0\n",
    "    Minimum sum cutoff of sample across all features.\n",
    "    The value can be at minimum zero and must be an\n",
    "    whole integer. It is suggested to be greater than\n",
    "    or equal to 500.\n",
    "\n",
    "    min_feature_count: int, optional : Default is 0\n",
    "    Minimum sum cutoff of features across all samples.\n",
    "    The value can be at minimum zero and must be\n",
    "    an whole integer.\n",
    "\n",
    "    min_feature_frequency: float, optional : Default is 0\n",
    "    Minimum percentage of samples a feature must appear\n",
    "    with a value greater than zero. This value can range\n",
    "    from 0 to 100 with decimal values allowed.\n",
    "\n",
    "    max_iterations: int, optional : Default is 5\n",
    "    The number of convex iterations to optimize the solution\n",
    "    If iteration is not specified, then the default iteration is 5.\n",
    "    Which reduces to a satisfactory error threshold.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    OrdinationResults\n",
    "        A biplot of the (Robust Aitchison) RPCA feature loadings\n",
    "\n",
    "    DistanceMatrix\n",
    "        The Aitchison distance of the sample loadings from RPCA.\n",
    "\n",
    "    TreeNode\n",
    "        The input tree with all nodes matched in name to the\n",
    "        features in the counts-by-node table\n",
    "\n",
    "    biom.Table\n",
    "        A table with all tree internal nodes as features with the\n",
    "        sum of all children of that node (i.e. FastUniFrac).\n",
    "\n",
    "    Optional[pd.DataFrame]\n",
    "        The resulting tax2Tree taxonomy and will include taxonomy for both\n",
    "        internal nodes and tips. Note: this will only be output\n",
    "        if taxonomy was given as input.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        `ValueError: n_components must be at least 2`.\n",
    "\n",
    "    ValueError\n",
    "        `ValueError: max_iterations must be at least 1`.\n",
    "\n",
    "    ValueError\n",
    "        `ValueError: Data-table contains either np.inf or -np.inf`.\n",
    "\n",
    "    ValueError\n",
    "        `ValueError: The n_components must be less\n",
    "            than the minimum shape of the input table`.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Martino C, Morton JT, Marotz CA, Thompson LR, Tripathi A,\n",
    "           Knight R, Zengler K. 2019. A Novel Sparse Compositional\n",
    "           Technique Reveals Microbial Perturbations. mSystems 4.\n",
    "    .. [2] Keshavan RH, Oh S, Montanari A. 2009. Matrix completion\n",
    "            from a few entries (2009_ IEEE International\n",
    "            Symposium on Information Theory\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from biom import Table\n",
    "    from gemelli.rpca import phylogenetic_rpca\n",
    "\n",
    "    # make a table\n",
    "    X = np.array([[9, 3, 0, 0],\n",
    "                [9, 9, 0, 1],\n",
    "                [0, 1, 4, 5],\n",
    "                [0, 0, 3, 4],\n",
    "                [1, 0, 8, 9]])\n",
    "    sample_ids = ['s1','s2','s3','s4']\n",
    "    feature_ids = ['f1','f2','f3','f4','f5']\n",
    "    bt = Table(X, feature_ids, sample_ids)\n",
    "    # write an example tree to read\n",
    "    f = open(\"demo-tree.nwk\", \"w\")\n",
    "    newick = '(((f1:1,f2:1)n9:1,f3:1)n8:1,(f4:1,f5:1)n2:1)n1:1;'\n",
    "    f.write(newick)\n",
    "    f.close()\n",
    "    # run RPCA without taxonomy\n",
    "    # s1/s2 will seperate from s3/s4\n",
    "    (ordination, distance_matrix,\n",
    "    tree, phylo_table, _) = phylogenetic_rpca(bt, 'demo-tree.nwk')\n",
    "\n",
    "    # make mock taxonomy\n",
    "    taxonomy = pd.DataFrame({fid:['k__kingdom; p__phylum;'\n",
    "                                'c__class; o__order; '\n",
    "                                'f__family; g__genus;'\n",
    "                                's__',\n",
    "                                0.99]\n",
    "                            for fid in feature_ids},\n",
    "                            ['Taxon', 'Confidence']).T\n",
    "    # run RPCA with taxonomy\n",
    "    # s1/s2 will seperate from s3/s4\n",
    "    (ordination, distance_matrix,\n",
    "    tree, phylo_table,\n",
    "    lca_taxonomy) = phylogenetic_rpca(bt, 'demo-tree.nwk', taxonomy)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # validate the metadata using q2 as a wrapper\n",
    "    if taxonomy is not None and not isinstance(taxonomy,\n",
    "                                               pd.DataFrame):\n",
    "        taxonomy = taxonomy.to_dataframe()\n",
    "    # use helper to process table\n",
    "    table = rpca_table_processing(table,\n",
    "                                  min_sample_count,\n",
    "                                  min_feature_count,\n",
    "                                  min_feature_frequency)\n",
    "\n",
    "    # import the tree based on filtered table\n",
    "    phylogeny = bp_read_phylogeny(table,\n",
    "                                  phylogeny,\n",
    "                                  min_depth)\n",
    "    # build the vectorized table\n",
    "    counts_by_node, tree_index, branch_lengths, fids, otu_ids\\\n",
    "        = fast_unifrac(table, phylogeny)\n",
    "    # Robust-clt (matrix_rclr) preprocessing\n",
    "    rclr_table = matrix_rclr(counts_by_node, branch_lengths=branch_lengths)\n",
    "    # run OptSpace (RPCA)\n",
    "    ord_res, dist_res = optspace_helper(rclr_table, fids, table.ids(),\n",
    "                                        n_components=n_components)\n",
    "    # import expanded table\n",
    "    counts_by_node = biom.Table(counts_by_node.T,\n",
    "                                fids, table.ids())\n",
    "    result_taxonomy = None\n",
    "    if taxonomy is not None:\n",
    "        # collect taxonomic information for all tree nodes.\n",
    "        traversed_taxonomy = retrieve_t2t_taxonomy(phylogeny, taxonomy)\n",
    "        result_taxonomy = create_taxonomy_metadata(phylogeny,\n",
    "                                                   traversed_taxonomy)\n",
    "\n",
    "    return ord_res, dist_res, phylogeny, counts_by_node, result_taxonomy\n",
    "\n",
    "\n",
    "def rpca(table: biom.Table,\n",
    "         n_components: Union[int, str] = DEFAULT_COMP,\n",
    "         min_sample_count: int = DEFAULT_MSC,\n",
    "         min_feature_count: int = DEFAULT_MFC,\n",
    "         min_feature_frequency: float = DEFAULT_MFF,\n",
    "         max_iterations: int = DEFAULT_OPTSPACE_ITERATIONS) -> (\n",
    "        OrdinationResults,\n",
    "        DistanceMatrix):\n",
    "    \"\"\"\n",
    "    Performs robust center log-ratio transform and\n",
    "    robust PCA. The robust PCA and enter log-ratio transform\n",
    "    operate on only observed values of the data.\n",
    "    For more information see (1 and 2).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    table: numpy.ndarray, required\n",
    "    The feature table in biom format containing the\n",
    "    samples over which metric should be computed.\n",
    "\n",
    "    n_components: int, optional : Default is 3\n",
    "    The underlying rank of the data and number of\n",
    "    output dimensions.\n",
    "\n",
    "    min_sample_count: int, optional : Default is 0\n",
    "    Minimum sum cutoff of sample across all features.\n",
    "    The value can be at minimum zero and must be an\n",
    "    whole integer. It is suggested to be greater than\n",
    "    or equal to 500.\n",
    "\n",
    "    min_feature_count: int, optional : Default is 0\n",
    "    Minimum sum cutoff of features across all samples.\n",
    "    The value can be at minimum zero and must be\n",
    "    an whole integer.\n",
    "\n",
    "    min_feature_frequency: float, optional : Default is 0\n",
    "    Minimum percentage of samples a feature must appear\n",
    "    with a value greater than zero. This value can range\n",
    "    from 0 to 100 with decimal values allowed.\n",
    "\n",
    "    max_iterations: int, optional : Default is 5\n",
    "    The number of convex iterations to optimize the solution\n",
    "    If iteration is not specified, then the default iteration is 5.\n",
    "    Which reduces to a satisfactory error threshold.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    OrdinationResults\n",
    "        A biplot of the (Robust Aitchison) RPCA feature loadings\n",
    "\n",
    "    DistanceMatrix\n",
    "        The Aitchison distance of the sample loadings from RPCA.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        `ValueError: n_components must be at least 2`.\n",
    "\n",
    "    ValueError\n",
    "        `ValueError: max_iterations must be at least 1`.\n",
    "\n",
    "    ValueError\n",
    "        `ValueError: Data-table contains either np.inf or -np.inf`.\n",
    "\n",
    "    ValueError\n",
    "        `ValueError: The n_components must be less\n",
    "            than the minimum shape of the input table`.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Martino C, Morton JT, Marotz CA, Thompson LR, Tripathi A,\n",
    "           Knight R, Zengler K. 2019. A Novel Sparse Compositional\n",
    "           Technique Reveals Microbial Perturbations. mSystems 4.\n",
    "    .. [2] Keshavan RH, Oh S, Montanari A. 2009. Matrix completion\n",
    "            from a few entries (2009_ IEEE International\n",
    "            Symposium on Information Theory\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    import numpy as np\n",
    "    from biom import Table\n",
    "    from gemelli.rpca import rpca\n",
    "\n",
    "    # make a table\n",
    "    X = np.array([[9, 3, 0, 0],\n",
    "                [9, 9, 0, 1],\n",
    "                [0, 1, 4, 5],\n",
    "                [0, 0, 3, 4],\n",
    "                [1, 0, 8, 9]])\n",
    "    sample_ids = ['s1','s2','s3','s4']\n",
    "    feature_ids = ['f1','f2','f3','f4','f5']\n",
    "    bt = Table(X, feature_ids, sample_ids)\n",
    "    # run RPCA (s1/s2 will seperate from s3/s4)\n",
    "    ordination, distance_matrix = rpca(bt)\n",
    "\n",
    "    \"\"\"\n",
    "    # use helper to process table\n",
    "    table = rpca_table_processing(table,\n",
    "                                  min_sample_count,\n",
    "                                  min_feature_count,\n",
    "                                  min_feature_frequency)\n",
    "    # Robust-clt (matrix_rclr) preprocessing\n",
    "    rclr_table = matrix_rclr(table.matrix_data.toarray().T)\n",
    "    # run OptSpace (RPCA)\n",
    "    ord_res, dist_res = optspace_helper(rclr_table,\n",
    "                                        table.ids('observation'),\n",
    "                                        table.ids(), n_components=n_components)\n",
    "\n",
    "    return ord_res, dist_res\n",
    "\n",
    "\n",
    "def rpca_with_cv(table: biom.Table,\n",
    "                 n_test_samples: int = DEFAULT_TESTS,\n",
    "                 sample_metadata: pd.DataFrame = DEFAULT_METACV,\n",
    "                 train_test_column: str = DEFAULT_COLCV,\n",
    "                 n_components: Union[int, str] = DEFAULT_COMP,\n",
    "                 max_iterations: int = DEFAULT_OPTSPACE_ITERATIONS,\n",
    "                 min_sample_count: int = DEFAULT_MSC,\n",
    "                 min_feature_count: int = DEFAULT_MFC,\n",
    "                 min_feature_frequency: float = DEFAULT_MFF) -> (\n",
    "                 OrdinationResults,\n",
    "                 DistanceMatrix,\n",
    "                 pd.DataFrame):\n",
    "    \"\"\"\n",
    "    RPCA but with CV used in Joint-RPCA.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    table: numpy.ndarray, required\n",
    "    The feature table in biom format containing the\n",
    "    samples over which metric should be computed.\n",
    "\n",
    "    n_test_samples: int, optional : Default is 10\n",
    "    Number of random samples to choose for test split samples.\n",
    "\n",
    "    metadata: DataFrame, optional : Default is None\n",
    "    Sample metadata file in QIIME2 formatting. The file must\n",
    "    contain a train-test column with labels `train` and `test`\n",
    "    and the row ids matched to the table(s).\n",
    "\n",
    "    train_test_column: str, optional : Default is None\n",
    "    Sample metadata column containing `train` and `test`\n",
    "    labels to use for the cross-validation evaluation.\n",
    "\n",
    "    n_components: int, optional : Default is 3\n",
    "    The underlying rank of the data and number of\n",
    "    output dimensions.\n",
    "\n",
    "    min_sample_count: int, optional : Default is 0\n",
    "    Minimum sum cutoff of sample across all features.\n",
    "    The value can be at minimum zero and must be an\n",
    "    whole integer. It is suggested to be greater than\n",
    "    or equal to 500.\n",
    "\n",
    "    min_feature_count: int, optional : Default is 0\n",
    "    Minimum sum cutoff of features across all samples.\n",
    "    The value can be at minimum zero and must be\n",
    "    an whole integer.\n",
    "\n",
    "    min_feature_frequency: float, optional : Default is 0\n",
    "    Minimum percentage of samples a feature must appear\n",
    "    with a value greater than zero. This value can range\n",
    "    from 0 to 100 with decimal values allowed.\n",
    "\n",
    "    max_iterations: int, optional : Default is 5\n",
    "    The number of convex iterations to optimize the solution\n",
    "    If iteration is not specified, then the default iteration is 5.\n",
    "    Which reduces to a satisfactory error threshold.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    OrdinationResults\n",
    "        A biplot of the (Robust Aitchison) RPCA feature loadings\n",
    "\n",
    "    DistanceMatrix\n",
    "        The Aitchison distance of the sample loadings from RPCA.\n",
    "\n",
    "    DataFrame\n",
    "        The cross-validation reconstruction error.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        `ValueError: n_components must be at least 2`.\n",
    "\n",
    "    ValueError\n",
    "        `ValueError: max_iterations must be at least 1`.\n",
    "\n",
    "    ValueError\n",
    "        `ValueError: Data-table contains either np.inf or -np.inf`.\n",
    "\n",
    "    ValueError\n",
    "        `ValueError: The n_components must be less\n",
    "            than the minimum shape of the input table`.\n",
    "\n",
    "    \"\"\"\n",
    "    res_tmp = joint_rpca([table],\n",
    "                         n_test_samples=n_test_samples,\n",
    "                         sample_metadata=sample_metadata,\n",
    "                         train_test_column=train_test_column,\n",
    "                         n_components=n_components,\n",
    "                         max_iterations=max_iterations,\n",
    "                         min_sample_count=min_sample_count,\n",
    "                         min_feature_count=min_feature_count,\n",
    "                         min_feature_frequency=min_feature_frequency)\n",
    "    ord_res, dist_res, cv_dist = res_tmp\n",
    "    return ord_res, dist_res, cv_dist\n",
    "\n",
    "\n",
    "def optspace_helper(rclr_table: np.array,\n",
    "                    feature_ids: list,\n",
    "                    subject_ids: list,\n",
    "                    n_components: Union[int, str] = DEFAULT_COMP,\n",
    "                    max_iterations: int = DEFAULT_OPTSPACE_ITERATIONS) -> (\n",
    "                        OrdinationResults,\n",
    "                        DistanceMatrix):\n",
    "    \"\"\"\n",
    "    Helper function. Please use rpca directly.\n",
    "    \"\"\"\n",
    "    # run OptSpace (RPCA)\n",
    "    opt = MatrixCompletion(n_components=n_components,\n",
    "                           max_iterations=max_iterations).fit(rclr_table)\n",
    "    # get new n-comp when applicable\n",
    "    n_components = opt.s.shape[0]\n",
    "    # get PC column labels for the skbio OrdinationResults\n",
    "    rename_cols = ['PC' + str(i + 1) for i in range(n_components)]\n",
    "    # get completed matrix for centering\n",
    "    X = opt.sample_weights @ opt.s @ opt.feature_weights.T\n",
    "    # center again around zero after completion\n",
    "    X = X - X.mean(axis=0)\n",
    "    X = X - X.mean(axis=1).reshape(-1, 1)\n",
    "    # re-factor the data\n",
    "    u, s, v = svd(X)\n",
    "    # only take n-components\n",
    "    u = u[:, :n_components]\n",
    "    v = v.T[:, :n_components]\n",
    "    # calc. the new variance using projection\n",
    "    p = s**2 / np.sum(s**2)\n",
    "    p = p[:n_components]\n",
    "    s = s[:n_components]\n",
    "    # save the loadings\n",
    "    feature_loading = pd.DataFrame(v, index=feature_ids,\n",
    "                                   columns=rename_cols)\n",
    "    sample_loading = pd.DataFrame(u, index=subject_ids,\n",
    "                                  columns=rename_cols)\n",
    "    # % var explained\n",
    "    proportion_explained = pd.Series(p, index=rename_cols)\n",
    "    # get eigenvalues\n",
    "    eigvals = pd.Series(s, index=rename_cols)\n",
    "\n",
    "    # if the n_components is two add PC3 of zeros\n",
    "    # this is referenced as in issue in\n",
    "    # <https://github.com/biocore/emperor/commit\n",
    "    # /a93f029548c421cb0ba365b4294f7a5a6b0209ce>\n",
    "    # discussed in gemelli -- PR#29\n",
    "    if n_components == 2:\n",
    "        feature_loading['PC3'] = [0] * len(feature_loading.index)\n",
    "        sample_loading['PC3'] = [0] * len(sample_loading.index)\n",
    "        eigvals.loc['PC3'] = 0\n",
    "        proportion_explained.loc['PC3'] = 0\n",
    "\n",
    "    # save ordination results\n",
    "    short_method_name = 'rpca_biplot'\n",
    "    long_method_name = '(Robust Aitchison) RPCA Biplot'\n",
    "    ord_res = skbio.OrdinationResults(\n",
    "        short_method_name,\n",
    "        long_method_name,\n",
    "        eigvals.copy(),\n",
    "        samples=sample_loading.copy(),\n",
    "        features=feature_loading.copy(),\n",
    "        proportion_explained=proportion_explained.copy())\n",
    "    # save distance matrix\n",
    "    dist_res = DistanceMatrix(opt.distance, ids=sample_loading.index)\n",
    "\n",
    "    return ord_res, dist_res\n",
    "\n",
    "\n",
    "def rpca_table_processing(table: biom.Table,\n",
    "                          min_sample_count: int = DEFAULT_MSC,\n",
    "                          min_feature_count: int = DEFAULT_MFC,\n",
    "                          min_feature_frequency: float = DEFAULT_MFF) -> (\n",
    "                              biom.Table):\n",
    "    \"\"\"Filter and checks the table validity for RPCA.\n",
    "    \"\"\"\n",
    "    # get shape of table\n",
    "    n_features, n_samples = table.shape\n",
    "\n",
    "    # filter sample to min seq. depth\n",
    "    def sample_filter(val, id_, md):\n",
    "        return sum(val) > min_sample_count\n",
    "\n",
    "    # filter features to min total counts\n",
    "    def observation_filter(val, id_, md):\n",
    "        return sum(val) > min_feature_count\n",
    "\n",
    "    # filter features by N samples presence\n",
    "    def frequency_filter(val, id_, md):\n",
    "        return (np.sum(val > 0) / n_samples) > (min_feature_frequency / 100)\n",
    "\n",
    "    # filter and import table for each filter above\n",
    "    if min_feature_count is not None:\n",
    "        table = table.filter(observation_filter,\n",
    "                             axis='observation',\n",
    "                             inplace=False)\n",
    "    if min_feature_frequency is not None:\n",
    "        table = table.filter(frequency_filter,\n",
    "                             axis='observation',\n",
    "                             inplace=False)\n",
    "    if min_sample_count is not None:\n",
    "        table = table.filter(sample_filter,\n",
    "                             axis='sample',\n",
    "                             inplace=False)\n",
    "    # check the table after filtering\n",
    "    if len(table.ids()) != len(set(table.ids())):\n",
    "        raise ValueError('Data-table contains duplicate sample IDs')\n",
    "    if len(table.ids('observation')) != len(set(table.ids('observation'))):\n",
    "        raise ValueError('Data-table contains duplicate feature IDs')\n",
    "    # ensure empty samples / features are removed\n",
    "    table = table.remove_empty()\n",
    "\n",
    "    return table\n",
    "\n",
    "\n",
    "def joint_rpca(tables: biom.Table,\n",
    "               n_test_samples: int = DEFAULT_TESTS,\n",
    "               sample_metadata: pd.DataFrame = DEFAULT_METACV,\n",
    "               train_test_column: str = DEFAULT_COLCV,\n",
    "               n_components: Union[int, str] = DEFAULT_COMP,\n",
    "               rclr_transform_tables: bool = DEFAULT_TRNSFRM,\n",
    "               min_sample_count: int = DEFAULT_MSC,\n",
    "               min_feature_count: int = DEFAULT_MFC,\n",
    "               min_feature_frequency: float = DEFAULT_MFF,\n",
    "               max_iterations: int = DEFAULT_OPTSPACE_ITERATIONS) -> (\n",
    "        OrdinationResults,\n",
    "        DistanceMatrix,\n",
    "        pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Performs joint-RPCA across data tables\n",
    "    with shared samples.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tables: list of biom.Table, required\n",
    "    A list of feature table in biom format containing shared\n",
    "    samples over which metric should be computed.\n",
    "\n",
    "    n_test_samples: int, optional : Default is 10\n",
    "    Number of random samples to choose for test split samples.\n",
    "\n",
    "    metadata: DataFrame, optional : Default is None\n",
    "    Sample metadata file in QIIME2 formatting. The file must\n",
    "    contain a train-test column with labels `train` and `test`\n",
    "    and the row ids matched to the table(s).\n",
    "\n",
    "    train_test_column: str, optional : Default is None\n",
    "    Sample metadata column containing `train` and `test`\n",
    "    labels to use for the cross-validation evaluation.\n",
    "\n",
    "    n_components: int, optional : Default is 3\n",
    "    The underlying rank of the data and number of\n",
    "    output dimensions.\n",
    "\n",
    "    rclr_transform_tables: bool, optional: If False Joint-RPCA\n",
    "    will not use the RCLR transformation and will instead\n",
    "    assume that the data has already been transformed\n",
    "    or normalized. Default is True.\n",
    "\n",
    "    max_iterations: int, optional : Default is 5\n",
    "    The number of convex iterations to optimize the solution\n",
    "    If iteration is not specified, then the default iteration is 5.\n",
    "    Which reduces to a satisfactory error threshold.\n",
    "\n",
    "    min_sample_count: int, optional : Default is 0\n",
    "    Minimum sum cutoff of sample across all features.\n",
    "    The value can be at minimum zero and must be an\n",
    "    whole integer. It is suggested to be greater than\n",
    "    or equal to 500.\n",
    "\n",
    "    min_feature_count: int, optional : Default is 0\n",
    "    Minimum sum cutoff of features across all samples.\n",
    "    The value can be at minimum zero and must be\n",
    "    an whole integer.\n",
    "\n",
    "    min_feature_frequency: float, optional : Default is 0\n",
    "    Minimum percentage of samples a feature must appear\n",
    "    with a value greater than zero. This value can range\n",
    "    from 0 to 100 with decimal values allowed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    OrdinationResults\n",
    "        A joint-biplot of the (Robust Aitchison) RPCA feature loadings\n",
    "\n",
    "    DistanceMatrix\n",
    "        The Aitchison distance of the sample loadings from RPCA.\n",
    "\n",
    "    DataFrame\n",
    "        The cross-validation reconstruction error.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        `ValueError: n_components must be at least 2`.\n",
    "\n",
    "    ValueError\n",
    "        `ValueError: max_iterations must be at least 1`.\n",
    "\n",
    "    ValueError\n",
    "        `ValueError: Data-table contains either np.inf or -np.inf`.\n",
    "\n",
    "    ValueError\n",
    "        `ValueError: The n_components must be less\n",
    "            than the minimum shape of the input table`.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # filter each table\n",
    "    for n, table_n in enumerate(tables):\n",
    "        if rclr_transform_tables:\n",
    "            tables[n] = rpca_table_processing(table_n,\n",
    "                                              min_sample_count,\n",
    "                                              min_feature_count,\n",
    "                                              min_feature_frequency)\n",
    "    # get set of shared samples\n",
    "    shared_all_samples = set.intersection(*[set(table_n.ids())\n",
    "                                            for table_n in tables])\n",
    "    # check sample overlaps\n",
    "    if len(shared_all_samples) == 0:\n",
    "        raise ValueError('No samples overlap between all tables. '\n",
    "                         'If using pre-transformed or normalized '\n",
    "                         'tables, make sure the rclr_transform_tables '\n",
    "                         'is set to False or the flag is enabled.')\n",
    "    unshared_samples = set([s_n\n",
    "                            for table_n in tables\n",
    "                            for s_n in table_n.ids()]) - shared_all_samples\n",
    "    if len(unshared_samples) != 0:\n",
    "        warnings.warn('Removing %i sample(s) that do not overlap in tables.'\n",
    "                      % (len(unshared_samples)), RuntimeWarning)\n",
    "    # filter each table again to subset samples.\n",
    "    for n, table_n in enumerate(tables):\n",
    "        if rclr_transform_tables:\n",
    "            table_n = table_n.filter(shared_all_samples)\n",
    "            tables[n] = rpca_table_processing(table_n,\n",
    "                                              min_sample_count,\n",
    "                                              min_feature_count,\n",
    "                                              min_feature_frequency)\n",
    "        else:\n",
    "            tables[n] = table_n.filter(shared_all_samples)\n",
    "    shared_all_samples = set.intersection(*[set(table_n.ids())\n",
    "                                            for table_n in tables])\n",
    "    # rclr each table\n",
    "    rclr_tables = []\n",
    "    for table_n in tables:\n",
    "        # perform RCLR\n",
    "        if rclr_transform_tables:\n",
    "            rclr_tmp = matrix_rclr(table_n.matrix_data.toarray().T).T\n",
    "        # otherwise just mask zeros\n",
    "        else:\n",
    "            rclr_tmp = mask_value_only(table_n.matrix_data.toarray().T).T\n",
    "        rclr_tables.append(pd.DataFrame(rclr_tmp,\n",
    "                                        table_n.ids('observation'),\n",
    "                                        table_n.ids()))\n",
    "    # get training and test sample IDs\n",
    "    if sample_metadata is not None and not isinstance(sample_metadata,\n",
    "                                                      pd.DataFrame):\n",
    "        sample_metadata = sample_metadata.to_dataframe()\n",
    "    if sample_metadata is None or train_test_column is None:\n",
    "        test_samples = sorted(list(shared_all_samples))[:n_test_samples]\n",
    "        train_samples = list(set(shared_all_samples) - set(test_samples))\n",
    "    else:\n",
    "        sample_metadata = sample_metadata.loc[shared_all_samples, :]\n",
    "        train_samples = sample_metadata[train_test_column] == 'train'\n",
    "        test_samples = sample_metadata[train_test_column] == 'test'\n",
    "        train_samples = sample_metadata[train_samples].index\n",
    "        test_samples = sample_metadata[test_samples].index\n",
    "    ord_res, U_dist_res, cv_dist = joint_optspace_helper(rclr_tables,\n",
    "                                                         n_components,\n",
    "                                                         max_iterations,\n",
    "                                                         test_samples,\n",
    "                                                         train_samples)\n",
    "    return ord_res, U_dist_res, cv_dist\n",
    "\n",
    "\n",
    "def joint_optspace_helper(tables,\n",
    "                          n_components,\n",
    "                          max_iterations,\n",
    "                          test_samples,\n",
    "                          train_samples):\n",
    "    \"\"\"\n",
    "    Helper function for joint-RPCA\n",
    "    \"\"\"\n",
    "\n",
    "    # split the tables by training and test samples\n",
    "    tables_split = [[table_i.loc[:, test_samples].T,\n",
    "                     table_i.loc[:, train_samples].T]\n",
    "                    for table_i in tables]\n",
    "    # run OptSpace\n",
    "    opt_model = OptSpace(n_components=n_components,\n",
    "                         max_iterations=max_iterations,\n",
    "                         tol=None)\n",
    "    U, s, Vs, dists = opt_model.joint_solve([[t_s.values for t_s in t]\n",
    "                                             for t in tables_split])\n",
    "    rename_cols = ['PC' + str(i + 1) for i in range(n_components)]\n",
    "    vjoint = pd.concat([pd.DataFrame(Vs_n,\n",
    "                                     index=t_n.index,\n",
    "                                     columns=rename_cols)\n",
    "                        for t_n, Vs_n in zip(tables, Vs)])\n",
    "    ujoint = pd.DataFrame(U,\n",
    "                          index=list(train_samples),\n",
    "                          columns=rename_cols)\n",
    "    # center again around zero after completion\n",
    "    X = ujoint.values @ s @ vjoint.values.T\n",
    "    X = X - X.mean(axis=0)\n",
    "    X = X - X.mean(axis=1).reshape(-1, 1)\n",
    "    u, s_new, v = svd(X, full_matrices=False)\n",
    "    s_eig = s_new[:n_components]\n",
    "    rename_cols = ['PC' + str(i + 1) for i in range(n_components)]\n",
    "    v = v.T[:, :n_components]\n",
    "    u = u[:, :n_components]\n",
    "    # create ordination\n",
    "    vjoint = pd.DataFrame(v,\n",
    "                          index=vjoint.index,\n",
    "                          columns=vjoint.columns)\n",
    "    ujoint = pd.DataFrame(u,\n",
    "                          index=list(train_samples),\n",
    "                          columns=ujoint.columns)\n",
    "    p = s_eig**2 / np.sum(s_eig**2)\n",
    "    eigvals = pd.Series(s_eig, index=rename_cols)\n",
    "    proportion_explained = pd.Series(p, index=rename_cols)\n",
    "    ord_res = OrdinationResults(\n",
    "            'rpca',\n",
    "            'rpca',\n",
    "            eigvals.copy(),\n",
    "            samples=ujoint.copy(),\n",
    "            features=vjoint.copy(),\n",
    "            proportion_explained=proportion_explained.copy())\n",
    "    # project test data into training data\n",
    "    if len(test_samples) > 0:\n",
    "        ord_res = transform(ord_res,\n",
    "                            [t[0] for t in tables_split],\n",
    "                            rclr_transform=False)\n",
    "    # save results\n",
    "    Udist = distance.cdist(ord_res.samples.copy(),\n",
    "                           ord_res.samples.copy())\n",
    "    U_dist_res = DistanceMatrix(Udist, ids=ord_res.samples.index)\n",
    "    cv_dist = pd.DataFrame(dists, ['mean_CV', 'std_CV']).T\n",
    "    cv_dist['run'] = 'tables_%i.n_components_%i.max_iterations_%i.n_test_%i' \\\n",
    "                     % (len(tables), n_components,\n",
    "                        max_iterations, len(test_samples))\n",
    "    cv_dist['iteration'] = list(cv_dist.index.astype(int))\n",
    "    cv_dist.index.name = 'sampleid'\n",
    "\n",
    "    return ord_res, U_dist_res, cv_dist\n",
    "\n",
    "\n",
    "def transform(ordination: OrdinationResults,\n",
    "              tables: biom.Table,\n",
    "              subset_tables: bool = DEFAULT_MATCH,\n",
    "              rclr_transform: bool = DEFAULT_TRNSFRM) -> (\n",
    "        OrdinationResults):\n",
    "    \"\"\"\n",
    "    Function to apply dimensionality reduction to table(s).\n",
    "    The table(s) is projected on the first principal components\n",
    "    previously extracted from a training set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ordination: OrdinationResults\n",
    "        A joint-biplot of the (Robust Aitchison) RPCA feature loadings\n",
    "        produced from the training data.\n",
    "\n",
    "    tables: list of biom.Table, required\n",
    "        A list of at least one feature table in biom format containing\n",
    "        shared samples over which metric should be computed.\n",
    "\n",
    "    subset_tables: bool, optional : default is True\n",
    "        Subsets the input tables to contain only features used in the\n",
    "        training data. If set to False and the tables are not perfectly\n",
    "        matched a ValueError will be produced.\n",
    "\n",
    "    rclr_transform: bool, optional : default is True\n",
    "        If set to false the function will expect `tables` to be dataframes\n",
    "        already rclr transformed. This is used for internal functionality\n",
    "        in the joint-rpca function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    OrdinationResults\n",
    "        A joint-biplot of the (Robust Aitchison) RPCA feature loadings\n",
    "        with both the input training data and new test data.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        `ValueError: The input tables do not contain all\n",
    "        the features in the ordination.`.\n",
    "\n",
    "    ValueError\n",
    "        `ValueError: Removing # features(s) in table(s)\n",
    "        but not the ordination.`.\n",
    "\n",
    "    ValueError\n",
    "        `ValueError: Features in the input table(s) not in\n",
    "        the features in the ordination.  Either set subset_tables to\n",
    "        True or match the tables to the ordination.`.\n",
    "    \"\"\"\n",
    "\n",
    "    # extract current U & V matrix\n",
    "    Udf = ordination.samples.copy()\n",
    "    Vdf = ordination.features.copy()\n",
    "    s_eig = ordination.eigvals.copy().values\n",
    "    # rclr each table [if needed]\n",
    "    rclr_table_df = []\n",
    "    if rclr_transform:\n",
    "        for table_n in tables:\n",
    "            rclr_tmp = matrix_rclr(table_n.matrix_data.toarray().T)\n",
    "            rclr_table_df.append(pd.DataFrame(rclr_tmp,\n",
    "                                              table_n.ids(),\n",
    "                                              table_n.ids('observation')))\n",
    "    else:\n",
    "        for table_n in tables:\n",
    "            rclr_table_df.append(table_n)\n",
    "    rclr_table_df = pd.concat(rclr_table_df, axis=1).T\n",
    "    # ensure feature IDs match\n",
    "    shared_features = set(rclr_table_df.index) & set(Vdf.index)\n",
    "    if len(shared_features) < len(set(Vdf.index)):\n",
    "        raise ValueError('The input tables do not contain all'\n",
    "                         ' the features in the ordination.')\n",
    "    elif subset_tables:\n",
    "        unshared_N = len(set(rclr_table_df.index)) - len(shared_features)\n",
    "        warnings.warn('Removing %i features(s) in table(s)'\n",
    "                      ' but not the ordination.'\n",
    "                      % (unshared_N), RuntimeWarning)\n",
    "    else:\n",
    "        raise ValueError('Features in the input table(s) not in'\n",
    "                         ' the features in the ordination.'\n",
    "                         ' Either set subset_tables to True or'\n",
    "                         ' match the tables to the ordination.')\n",
    "    ordination.samples = transform_helper(Udf,\n",
    "                                          Vdf,\n",
    "                                          s_eig,\n",
    "                                          rclr_table_df)\n",
    "    return ordination\n",
    "\n",
    "\n",
    "def transform_helper(Udf, Vdf, s_eig, table_rclr_project):\n",
    "    # project new data into ordination\n",
    "    table_rclr_project = table_rclr_project.reindex(Vdf.index)\n",
    "    M_project = np.ma.array(table_rclr_project,\n",
    "                            mask=np.isnan(table_rclr_project)).T\n",
    "    M_project = M_project - M_project.mean(axis=1).reshape(-1, 1)\n",
    "    M_project = M_project - M_project.mean(axis=0)\n",
    "    U_projected = np.ma.dot(M_project, Vdf.values).data\n",
    "    U_projected /= np.linalg.norm(s_eig)\n",
    "    U_projected = pd.DataFrame(U_projected,\n",
    "                               table_rclr_project.columns,\n",
    "                               Udf.columns)\n",
    "    return pd.concat([Udf, U_projected])\n",
    "\n",
    "\n",
    "def rpca_transform(ordination: OrdinationResults,\n",
    "                   table: biom.Table,\n",
    "                   subset_tables: bool = DEFAULT_MATCH,\n",
    "                   rclr_transform: bool = DEFAULT_TRNSFRM) -> (\n",
    "        OrdinationResults):\n",
    "    \"\"\"\n",
    "    To avoid confusion this helper function takes one input\n",
    "    to use in QIIME2.\n",
    "    \"\"\"\n",
    "    ordination = transform(ordination, [table],\n",
    "                           subset_tables=subset_tables,\n",
    "                           rclr_transform=rclr_transform)\n",
    "    return ordination\n",
    "\n",
    "\n",
    "def feature_covariance_table(ordination, features_use=None):\n",
    "    \"\"\"\n",
    "    Function to produce a feature by feature\n",
    "    covariance table from RPCA ordination\n",
    "    results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ordination: OrdinationResults\n",
    "        A joint-biplot of the (Robust Aitchison) RPCA feature loadings\n",
    "    features_use: list, optional : default is None\n",
    "        A subset of features to use in the covariance generation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        A feature by feature covariance table.\n",
    "\n",
    "    \"\"\"\n",
    "    if features_use is not None:\n",
    "        vjoint = ordination.features.copy()\n",
    "        if len(set(features_use) - set(vjoint.index)) != 0:\n",
    "            raise ValueError('Feature subset given contains labels'\n",
    "                             ' not in the loadings.')\n",
    "        vjoint = vjoint.loc[features_use, :]\n",
    "    else:\n",
    "        vjoint = ordination.features\n",
    "    s = ordination.eigvals.values\n",
    "    Vs_joint = vjoint.values @ np.diag(s)**2 @ vjoint.values.T\n",
    "    joint_features = pd.DataFrame(Vs_joint,\n",
    "                                  vjoint.index,\n",
    "                                  vjoint.index)\n",
    "\n",
    "    return joint_features\n",
    "\n",
    "\n",
    "def feature_correlation_table(ordination: OrdinationResults) -> (pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Function to produce a feature by feature\n",
    "    correlation table from RPCA ordination\n",
    "    results. Note that the output can be very large in\n",
    "    file size because it is all omics features by all\n",
    "    omics features and is fully dense. If you would like to\n",
    "    get a subset, just subset the ordination with the function\n",
    "    `filter_ordination` in utils first.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ordination: OrdinationResults\n",
    "        A joint-biplot of the (Robust Aitchison) RPCA feature loadings.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        A feature by feature correlation table.\n",
    "\n",
    "    \"\"\"\n",
    "    joint_features = feature_covariance_table(ordination)\n",
    "    # this part of the function is taken from:\n",
    "    # https://gist.github.com/wiso/ce2a9919ded228838703c1c7c7dad13b\n",
    "    covariance = joint_features.values\n",
    "    v = np.sqrt(np.diag(covariance))\n",
    "    outer_v = np.outer(v, v)\n",
    "    correlation = covariance / outer_v\n",
    "    correlation[covariance == 0] = 0\n",
    "    # convert back to dataframe\n",
    "    correlation = pd.DataFrame(correlation,\n",
    "                               joint_features.index,\n",
    "                               joint_features.columns)\n",
    "    correlation.index.name = 'featureid'\n",
    "    return correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fe6139-fa3c-44e5-977c-0a8c8ce84854",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ede07f32-1243-414b-97f9-1da875917559",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "from copy import deepcopy\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "\n",
    "def binarize(base_matrix):\n",
    "\treturn np.where(base_matrix < 0, -1, 1)\n",
    "\n",
    "def encoding_rp(X_data, base_matrix, signed=False):\n",
    "\tenc_hvs = []\n",
    "\tfor i in range(len(X_data)):\n",
    "\t\tif i % int(len(X_data)/20) == 0:\n",
    "\t\t\tsys.stdout.write(str(int(i/len(X_data)*100)) + '% ')\n",
    "\t\t\tsys.stdout.flush()\n",
    "\t\thv = np.matmul(base_matrix, X_data[i])\n",
    "\t\tif signed:\n",
    "\t\t\thv = binarize(hv)\n",
    "\t\tenc_hvs.append(hv)\n",
    "\treturn enc_hvs\n",
    "\n",
    "def encoding_idlv(X_data, lvl_hvs, id_hvs, D, bin_len, x_min, L=64):\n",
    "\tenc_hvs = []\n",
    "\tfor i in range(len(X_data)):\n",
    "\t\tif i == int(len(X_data)/1):\n",
    "\t\t\tbreak\n",
    "\t\tif i % int(len(X_data)/20) == 0:\n",
    "\t\t\tsys.stdout.write(str(int(i/len(X_data)*100)) + '% ')\n",
    "\t\t\tsys.stdout.flush()\n",
    "\t\tsum_ = np.array([0] * D)\n",
    "\t\tfor j in range(len(X_data[i])):\n",
    "            # bin_ = min( np.round((X_data[i][j] - x_min)/bin_len), L-1)\n",
    "\t\t\tbin_ = min( np.floor((X_data[i][j] - x_min)/bin_len), L-1)\n",
    "\t\t\tbin_ = int(bin_)\n",
    "\t\t\tsum_ += lvl_hvs[bin_]*id_hvs[j]\n",
    "\t\tenc_hvs.append(sum_)\n",
    "\treturn enc_hvs\n",
    "\n",
    "def encoding_perm(X_data, lvl_hvs, D, bin_len, x_min, L=64):\n",
    "\tenc_hvs = []\n",
    "\tfor i in range(len(X_data)):\n",
    "\t\tif i % int(len(X_data)/20) == 0:\n",
    "\t\t\tsys.stdout.write(str(int(i/len(X_data)*100)) + '% ')\n",
    "\t\t\tsys.stdout.flush()\n",
    "\t\tsum_ = np.array([0] * D)\n",
    "\t\tfor j in range(len(X_data[i])):\n",
    "            # bin_ = min( np.round((X_data[i][j] - x_min)/bin_len), L-1)\n",
    "\t\t\tbin_ = min( np.floor((X_data[i][j] - x_min)/bin_len), L-1)\n",
    "\t\t\tbin_ = int(bin_)\n",
    "\t\t\tsum_ += np.roll(lvl_hvs[bin_], j)\n",
    "\t\tenc_hvs.append(sum_)\n",
    "\treturn enc_hvs\n",
    "\n",
    "def max_match(class_hvs, enc_hv, class_norms):\n",
    "\t\tmax_score = -np.inf\n",
    "\t\tmax_index = -1\n",
    "\t\tfor i in range(len(class_hvs)):\n",
    "\t\t\tscore = np.matmul(class_hvs[i], enc_hv) / class_norms[i]\n",
    "\t\t\t#score = np.matmul(class_hvs[i], enc_hv)\n",
    "\t\t\tif score > max_score:\n",
    "\t\t\t\tmax_score = score\n",
    "\t\t\t\tmax_index = i\n",
    "\t\treturn max_index\n",
    "\n",
    "def train(X_train, y_train, X_test, y_test, D=500, alg='rp', epoch=20, lr=1.0, L=64):\n",
    "\t\n",
    "    #randomly select 20% of train data as validation\n",
    "    permvar = np.arange(0, len(X_train))\n",
    "    np.random.shuffle(permvar)\n",
    "    X_train = [X_train[i] for i in permvar]\n",
    "    y_train = [y_train[i] for i in permvar]\n",
    "    cnt_vld = int(0.2 * len(X_train))\n",
    "    X_validation = X_train[0:cnt_vld]\n",
    "    y_validation = y_train[0:cnt_vld]\n",
    "    X_train = X_train[cnt_vld:]\n",
    "    y_train = y_train[cnt_vld:]\n",
    "    print(X_train)\n",
    "    print(y_train)\n",
    "    #encodings\n",
    "    if alg in ['rp', 'rp-sign']:\n",
    "        #create base matrix\n",
    "        base_matrix = np.random.rand(D, len(X_train[0]))\n",
    "        base_matrix = np.where(base_matrix > 0.5, 1, -1)\n",
    "        base_matrix = np.array(base_matrix, np.int8)\n",
    "        print('\\nEncoding ' + str(len(X_train)) + ' train data')\n",
    "        train_enc_hvs = encoding_rp(X_train, base_matrix, signed=(alg == 'rp-sign'))\n",
    "        print('\\n\\nEncoding ' + str(len(X_validation)) + ' validation data')\n",
    "        validation_enc_hvs = encoding_rp(X_validation, base_matrix, signed=(alg == 'rp-sign'))\n",
    "    \n",
    "    elif alg in ['idlv', 'perm']:\n",
    "        #create level matrix\n",
    "        lvl_hvs = []\n",
    "        temp = [-1]*int(D/2) + [1]*int(D/2)\n",
    "        np.random.shuffle(temp)\n",
    "        lvl_hvs.append(temp)\n",
    "        change_list = np.arange(0, D)\n",
    "        np.random.shuffle(change_list)\n",
    "        cnt_toChange = int(D/2 / (L-1))\n",
    "        for i in range(1, L):\n",
    "            temp = np.array(lvl_hvs[i-1])\n",
    "            temp[change_list[(i-1)*cnt_toChange : i*cnt_toChange]] = -temp[change_list[(i-1)*cnt_toChange : i*cnt_toChange]]\n",
    "            lvl_hvs.append(list(temp))\n",
    "        lvl_hvs = np.array(lvl_hvs, dtype=np.int8)\n",
    "        x_min = min( np.min(X_train), np.min(X_validation) )\n",
    "        x_max = max( np.max(X_train), np.max(X_validation) )\n",
    "        bin_len = (x_max - x_min)/float(L)\n",
    "        \n",
    "        #need to create id hypervectors if encoding is level-id\n",
    "        if alg == 'idlv':\n",
    "            cnt_id = len(X_train[0])\n",
    "            id_hvs = []\n",
    "            for i in range(cnt_id):\n",
    "                temp = [-1]*int(D/2) + [1]*int(D/2)\n",
    "                np.random.shuffle(temp)\n",
    "                id_hvs.append(temp)\n",
    "            id_hvs = np.array(id_hvs, dtype=np.int8)\n",
    "            print('\\nEncoding ' + str(len(X_train)) + ' train data')\n",
    "            train_enc_hvs = encoding_idlv(X_train, lvl_hvs, id_hvs, D, bin_len, x_min, L)\n",
    "            print('\\n\\nEncoding ' + str(len(X_validation)) + ' validation data')\n",
    "            validation_enc_hvs = encoding_idlv(X_validation, lvl_hvs, id_hvs, D, bin_len, x_min, L)\n",
    "        elif alg == 'perm':\n",
    "            print('\\nEncoding ' + str(len(X_train)) + ' train data')\n",
    "            train_enc_hvs = encoding_perm(X_train, lvl_hvs, D, bin_len, x_min, L)\n",
    "            print('\\n\\nEncoding ' + str(len(X_validation)) + ' validation data')\n",
    "            validation_enc_hvs = encoding_perm(X_validation, lvl_hvs, D, bin_len, x_min, L)\n",
    "\n",
    "\t#training, initial model\n",
    "    class_hvs = [[0.] * D] * (max(y_train) + 1)\n",
    "    for i in range(len(train_enc_hvs)):\n",
    "        class_hvs[y_train[i]] += train_enc_hvs[i]\n",
    "    class_norms = [np.linalg.norm(hv) for hv in class_hvs]\n",
    "    class_hvs_best = deepcopy(class_hvs)\n",
    "    class_norms_best = deepcopy(class_norms)\n",
    "    #retraining\n",
    "    if epoch > 0:\n",
    "        acc_max = -np.inf\n",
    "        print('\\n\\n' + str(epoch) + ' retraining epochs')\n",
    "        for i in range(epoch):\n",
    "            sys.stdout.write('epoch ' + str(i) + ': ')\n",
    "            sys.stdout.flush()\n",
    "            #shuffle data during retraining\n",
    "            pickList = np.arange(0, len(train_enc_hvs))\n",
    "            np.random.shuffle(pickList)\n",
    "            for j in pickList:\n",
    "                predict = max_match(class_hvs, train_enc_hvs[j], class_norms)\n",
    "                if predict != y_train[j]:\n",
    "                    class_hvs[predict] -= np.multiply(lr, train_enc_hvs[j])\n",
    "                    class_hvs[y_train[j]] += np.multiply(lr, train_enc_hvs[j])\n",
    "            class_norms = [np.linalg.norm(hv) for hv in class_hvs]\n",
    "            correct = 0\n",
    "            for j in range(len(validation_enc_hvs)):\n",
    "                predict = max_match(class_hvs, validation_enc_hvs[j], class_norms)\n",
    "                if predict == y_validation[j]:\n",
    "                    correct += 1\n",
    "            acc = float(correct)/len(validation_enc_hvs)\n",
    "            sys.stdout.write(\"%.4f \" %acc)\n",
    "            sys.stdout.flush()\n",
    "            if i > 0 and i%5 == 0:\n",
    "                print('')\n",
    "            if acc > acc_max:\n",
    "                acc_max = acc\n",
    "                class_hvs_best = deepcopy(class_hvs)\n",
    "                class_norms_best = deepcopy(class_norms)\n",
    "    \n",
    "    del X_train\n",
    "    del X_validation\n",
    "    del train_enc_hvs\n",
    "    del validation_enc_hvs\n",
    "    \n",
    "    print('\\n\\nEncoding ' + str(len(X_test)) + ' test data')\n",
    "    if alg == 'rp' or alg == 'rp-sign':\n",
    "        test_enc_hvs = encoding_rp(X_test, base_matrix, signed=(alg == 'rp-sign'))\n",
    "    elif alg == 'idlv':\n",
    "        test_enc_hvs = encoding_idlv(X_test, lvl_hvs, id_hvs, D, bin_len, x_min, L)\n",
    "    elif alg == 'perm':\n",
    "            test_enc_hvs = encoding_perm(X_test, lvl_hvs, D, bin_len, x_min, L)\n",
    "    correct = 0\n",
    "    for i in range(len(test_enc_hvs)):\n",
    "        predict = max_match(class_hvs_best, test_enc_hvs[i], class_norms_best)\n",
    "        if predict == y_test[i]:\n",
    "            correct += 1\n",
    "    acc = float(correct)/len(test_enc_hvs)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37d90b3a-cf64-4d64-8a27-2c7b0fe6e6ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 39\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# path = \"./train_data.csv\"\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m alg \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrp-sign\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124midlv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperm\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[43mpath\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     40\u001b[0m \tdataset \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\t\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'path' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#python main.py --path isolet.pickle --d 1000 --alg rp --epoch 20\n",
    "#python main.py --path isolet.pickle --d 1000 --alg rp-sign --epoch 20\n",
    "#python main.py --path isolet.pickle --d 1000 --alg idlv --epoch 20 --L 64 \n",
    "#python main.py --path isolet.pickle --d 1000 --alg perm --epoch 20 --L 64 \n",
    "\n",
    "# import HD\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "from copy import deepcopy\n",
    "import argparse\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# # parser.add_argument('--path', action='store', type=str, help='path to pickle dataset', required=True)\n",
    "# parser.add_argument('--d', action='store', dest='D', type=int, default=500, help='number of dimensions (default 500)')\n",
    "# parser.add_argument('--alg', action='store', type=str, default='rp', help='encoding technique (rp, rp-sign, idlv, perm')\n",
    "# parser.add_argument('--epoch', action='store', type=int, default=20, help='number of retraining iterations (default 20)')\n",
    "# parser.add_argument('--lr', '-lr', action='store', type=float, default=1.0, help='learning rate (default 1.0)')\n",
    "# parser.add_argument('--L', action='store', type=int, default=64, help='number of levels (default 64)')\n",
    "\n",
    "# inputs = parser.parse_args()\n",
    "# path = inputs.path\n",
    "# D = inputs.D\n",
    "# alg = inputs.alg\n",
    "# epoch = inputs.epoch\n",
    "# lr = inputs.lr\n",
    "# L = inputs.L\n",
    "D=500\n",
    "alg='rp'\n",
    "epoch=20\n",
    "lr=1.0\n",
    "L=64\n",
    "# path = \"./train_data.csv\"\n",
    "\n",
    "assert alg in ['rp', 'rp-sign', 'idlv', 'perm']\n",
    "\n",
    "with open(path, 'rb') as f:\n",
    "\tdataset = pickle.load(f, encoding='latin1')\t\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(pd.__version__)\n",
    "print(sklearn.__version__)\n",
    "\n",
    "train_data = train_dataset('train_data.csv', 'Meta.csv', 'apoe4')\n",
    "\n",
    "X_train, y_train, X_test, y_test = deepcopy(dataset)\n",
    "acc = HD.train(X_train, y_train, X_test, y_test, D, alg, epoch, lr, L)\n",
    "print('\\n')\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afad2af2-de7a-4505-8cd9-59f1324fc056",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
